#!/usr/bin/env python
"""
ç³»ç»Ÿç»´æŠ¤è„šæœ¬
æä¾›æ—¥å¸¸ç»´æŠ¤ã€æ¸…ç†ã€å¤‡ä»½ã€è¯Šæ–­ç­‰åŠŸèƒ½
"""

import os
import sys
import shutil
import json
import time
import tarfile
import zipfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional


class SystemMaintainer:
    """ç³»ç»Ÿç»´æŠ¤å™¨"""

    def __init__(self):
        self.project_root = Path.cwd()
        self.data_dir = self.project_root / "data/stock_data"
        self.logs_dir = self.project_root / "logs"
        self.backup_dir = self.project_root / "backups"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.logs_dir.mkdir(exist_ok=True)
        self.backup_dir.mkdir(exist_ok=True)

    def log(self, message: str, level: str = "INFO"):
        """è®°å½•æ—¥å¿—"""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_message = f"[{timestamp}] [{level}] {message}"
        print(log_message)

        # å†™å…¥ç»´æŠ¤æ—¥å¿—
        log_file = self.logs_dir / "maintenance.log"
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(log_message + "\n")

    def get_system_status(self) -> Dict[str, Any]:
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        status = {
            "timestamp": datetime.now().isoformat(),
            "disk_usage": {},
            "file_counts": {},
            "log_size": {},
            "database_size": {},
        }

        try:
            # ç£ç›˜ä½¿ç”¨æƒ…å†µ
            disk_usage = shutil.disk_usage(self.project_root)
            status["disk_usage"] = {
                "total_gb": round(disk_usage.total / (1024**3), 2),
                "used_gb": round(disk_usage.used / (1024**3), 2),
                "free_gb": round(disk_usage.free / (1024**3), 2),
                "usage_percent": round((disk_usage.used / disk_usage.total) * 100, 2),
            }

            # æ–‡ä»¶ç»Ÿè®¡
            if self.data_dir.exists():
                pdf_files = list(self.data_dir.glob("pdf_reports/*.pdf"))
                parsed_files = list(self.data_dir.glob("debug_data/*/*.json"))
                vector_files = list(self.data_dir.glob("databases/vector_dbs/*.faiss"))
                bm25_files = list(self.data_dir.glob("databases/bm25/*.pkl"))

                status["file_counts"] = {
                    "pdf_reports": len(pdf_files),
                    "parsed_files": len(parsed_files),
                    "vector_files": len(vector_files),
                    "bm25_files": len(bm25_files),
                }

            # æ—¥å¿—å¤§å°
            if self.logs_dir.exists():
                log_files = list(self.logs_dir.glob("*.log"))
                total_log_size = sum(f.stat().st_size for f in log_files)

                status["log_size"] = {
                    "total_mb": round(total_log_size / (1024**2), 2),
                    "file_count": len(log_files),
                }

            # æ•°æ®åº“å¤§å°
            if self.data_dir.exists():
                db_dirs = [
                    "databases/vector_dbs",
                    "databases/bm25",
                    "databases/chunked_reports",
                ]
                total_db_size = 0

                for db_dir in db_dirs:
                    db_path = self.data_dir / db_dir
                    if db_path.exists():
                        for file_path in db_path.rglob("*"):
                            if file_path.is_file():
                                total_db_size += file_path.stat().st_size

                status["database_size"] = {
                    "total_mb": round(total_db_size / (1024**2), 2)
                }

        except Exception as e:
            status["error"] = str(e)

        return status

    def clean_logs(self, keep_days: int = 7) -> Dict[str, Any]:
        """æ¸…ç†æ—¥å¿—æ–‡ä»¶"""
        self.log(f"æ¸…ç† {keep_days} å¤©å‰çš„æ—¥å¿—æ–‡ä»¶...")

        result = {"cleaned_files": [], "cleaned_size": 0, "error": None}

        try:
            cutoff_date = datetime.now() - timedelta(days=keep_days)

            for log_file in self.logs_dir.glob("*.log"):
                try:
                    # æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    file_time = datetime.fromtimestamp(log_file.stat().st_mtime)

                    if file_time < cutoff_date and log_file.name != "maintenance.log":
                        file_size = log_file.stat().st_size
                        log_file.unlink()

                        result["cleaned_files"].append(log_file.name)
                        result["cleaned_size"] += file_size

                        self.log(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶: {log_file.name}")

                except Exception as e:
                    self.log(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}", "ERROR")

            if result["cleaned_files"]:
                size_mb = round(result["cleaned_size"] / (1024**2), 2)
                self.log(
                    f"æ—¥å¿—æ¸…ç†å®Œæˆ: åˆ é™¤ {len(result['cleaned_files'])} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ {size_mb}MB"
                )
            else:
                self.log("æ²¡æœ‰éœ€è¦æ¸…ç†çš„æ—¥å¿—æ–‡ä»¶")

        except Exception as e:
            result["error"] = str(e)
            self.log(f"æ—¥å¿—æ¸…ç†å¤±è´¥: {e}", "ERROR")

        return result

    def clean_temp_files(self) -> Dict[str, Any]:
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        self.log("æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")

        result = {"cleaned_files": [], "cleaned_size": 0, "error": None}

        try:
            # è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶æ¨¡å¼
            temp_patterns = [
                "*.tmp",
                "*.temp",
                "*~",
                "*.bak",
                "*.pyc",
                "__pycache__",
                ".DS_Store",
                "Thumbs.db",
            ]

            for pattern in temp_patterns:
                for temp_file in self.project_root.rglob(pattern):
                    try:
                        if temp_file.is_file():
                            file_size = temp_file.stat().st_size
                            temp_file.unlink()

                            result["cleaned_files"].append(str(temp_file))
                            result["cleaned_size"] += file_size
                        elif temp_file.is_dir() and pattern == "__pycache__":
                            shutil.rmtree(temp_file)
                            result["cleaned_files"].append(str(temp_file))

                    except Exception as e:
                        self.log(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {e}", "WARNING")

            if result["cleaned_files"]:
                size_mb = round(result["cleaned_size"] / (1024**2), 2)
                self.log(
                    f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ: åˆ é™¤ {len(result['cleaned_files'])} ä¸ªé¡¹ç›®ï¼Œé‡Šæ”¾ {size_mb}MB"
                )
            else:
                self.log("æ²¡æœ‰éœ€è¦æ¸…ç†çš„ä¸´æ—¶æ–‡ä»¶")

        except Exception as e:
            result["error"] = str(e)
            self.log(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}", "ERROR")

        return result

    def backup_data(self, include_logs: bool = False) -> Dict[str, Any]:
        """å¤‡ä»½æ•°æ®"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"stock_rag_backup_{timestamp}"

        self.log(f"å¼€å§‹æ•°æ®å¤‡ä»½: {backup_name}")

        result = {
            "backup_file": None,
            "backup_size": 0,
            "included_items": [],
            "error": None,
        }

        try:
            backup_file = self.backup_dir / f"{backup_name}.tar.gz"

            with tarfile.open(backup_file, "w:gz") as tar:
                # å¤‡ä»½æ•°æ®ç›®å½•
                if self.data_dir.exists():
                    tar.add(self.data_dir, arcname="data")
                    result["included_items"].append("data/")

                # å¤‡ä»½é…ç½®æ–‡ä»¶
                config_files = [".env", "config_template.env", "requirements.txt"]
                for config_file in config_files:
                    file_path = self.project_root / config_file
                    if file_path.exists():
                        tar.add(file_path, arcname=config_file)
                        result["included_items"].append(config_file)

                # å¤‡ä»½æ—¥å¿—ï¼ˆå¦‚æœé€‰æ‹©ï¼‰
                if include_logs and self.logs_dir.exists():
                    tar.add(self.logs_dir, arcname="logs")
                    result["included_items"].append("logs/")

                # å¤‡ä»½é‡è¦è„šæœ¬
                important_scripts = [
                    "start_system.py",
                    "system_monitor.py",
                    "validate_config.py",
                    "deploy_system.py",
                ]

                for script in important_scripts:
                    script_path = self.project_root / script
                    if script_path.exists():
                        tar.add(script_path, arcname=script)
                        result["included_items"].append(script)

            # è·å–å¤‡ä»½æ–‡ä»¶å¤§å°
            result["backup_file"] = str(backup_file)
            result["backup_size"] = backup_file.stat().st_size

            size_mb = round(result["backup_size"] / (1024**2), 2)
            self.log(f"å¤‡ä»½å®Œæˆ: {backup_file.name} ({size_mb}MB)")

        except Exception as e:
            result["error"] = str(e)
            self.log(f"æ•°æ®å¤‡ä»½å¤±è´¥: {e}", "ERROR")

        return result

    def restore_backup(self, backup_file: str) -> Dict[str, Any]:
        """æ¢å¤å¤‡ä»½"""
        self.log(f"å¼€å§‹æ¢å¤å¤‡ä»½: {backup_file}")

        result = {"restored_items": [], "error": None}

        try:
            backup_path = Path(backup_file)
            if not backup_path.exists():
                # å°è¯•åœ¨å¤‡ä»½ç›®å½•ä¸­æŸ¥æ‰¾
                backup_path = self.backup_dir / backup_file

            if not backup_path.exists():
                raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")

            # åˆ›å»ºæ¢å¤å‰çš„å¤‡ä»½
            pre_restore_backup = self.backup_data(include_logs=True)
            if pre_restore_backup.get("backup_file"):
                self.log(f"æ¢å¤å‰å¤‡ä»½å·²åˆ›å»º: {pre_restore_backup['backup_file']}")

            with tarfile.open(backup_path, "r:gz") as tar:
                # è·å–å¤‡ä»½å†…å®¹
                members = tar.getnames()

                # æ¢å¤æ•°æ®
                tar.extractall(path=self.project_root)
                result["restored_items"] = members

            self.log(f"å¤‡ä»½æ¢å¤å®Œæˆ: æ¢å¤äº† {len(result['restored_items'])} ä¸ªé¡¹ç›®")

        except Exception as e:
            result["error"] = str(e)
            self.log(f"å¤‡ä»½æ¢å¤å¤±è´¥: {e}", "ERROR")

        return result

    def check_disk_space(self, warn_threshold: float = 80.0) -> Dict[str, Any]:
        """æ£€æŸ¥ç£ç›˜ç©ºé—´"""
        self.log("æ£€æŸ¥ç£ç›˜ç©ºé—´...")

        result = {"disk_usage": {}, "warnings": [], "recommendations": []}

        try:
            disk_usage = shutil.disk_usage(self.project_root)
            usage_percent = (disk_usage.used / disk_usage.total) * 100

            result["disk_usage"] = {
                "total_gb": round(disk_usage.total / (1024**3), 2),
                "used_gb": round(disk_usage.used / (1024**3), 2),
                "free_gb": round(disk_usage.free / (1024**3), 2),
                "usage_percent": round(usage_percent, 2),
            }

            if usage_percent > warn_threshold:
                warning = (
                    f"ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡é˜ˆå€¼: {usage_percent:.1f}% > {warn_threshold}%"
                )
                result["warnings"].append(warning)
                self.log(warning, "WARNING")

                # æä¾›å»ºè®®
                if usage_percent > 90:
                    result["recommendations"].extend(
                        [
                            "ç«‹å³æ¸…ç†ä¸´æ—¶æ–‡ä»¶",
                            "åˆ é™¤æ—§çš„æ—¥å¿—æ–‡ä»¶",
                            "å‹ç¼©æˆ–åˆ é™¤æ—§çš„å¤‡ä»½æ–‡ä»¶",
                            "è€ƒè™‘å¢åŠ ç£ç›˜ç©ºé—´",
                        ]
                    )
                else:
                    result["recommendations"].extend(
                        [
                            "æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œæ—¥å¿—",
                            "å®šæœŸå¤‡ä»½å¹¶åˆ é™¤æ—§æ•°æ®",
                            "ç›‘æ§ç£ç›˜ä½¿ç”¨è¶‹åŠ¿",
                        ]
                    )
            else:
                self.log(f"ç£ç›˜ç©ºé—´å……è¶³: {usage_percent:.1f}%")

        except Exception as e:
            result["error"] = str(e)
            self.log(f"ç£ç›˜ç©ºé—´æ£€æŸ¥å¤±è´¥: {e}", "ERROR")

        return result

    def optimize_databases(self) -> Dict[str, Any]:
        """ä¼˜åŒ–æ•°æ®åº“"""
        self.log("ä¼˜åŒ–æ•°æ®åº“...")

        result = {"optimized_items": [], "space_saved": 0, "error": None}

        try:
            # æ¸…ç†é‡å¤çš„å‘é‡æ–‡ä»¶
            vector_dir = self.data_dir / "databases/vector_dbs"
            if vector_dir.exists():
                vector_files = list(vector_dir.glob("*.faiss"))

                # æŒ‰æ–‡ä»¶å¤§å°åˆ†ç»„ï¼ŒæŸ¥æ‰¾å¯èƒ½çš„é‡å¤æ–‡ä»¶
                size_groups = {}
                for vf in vector_files:
                    size = vf.stat().st_size
                    if size not in size_groups:
                        size_groups[size] = []
                    size_groups[size].append(vf)

                for size, files in size_groups.items():
                    if len(files) > 1:
                        # ä¿ç•™æœ€æ–°çš„ï¼Œåˆ é™¤å…¶ä»–çš„
                        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
                        for duplicate in files[1:]:
                            result["space_saved"] += duplicate.stat().st_size
                            duplicate.unlink()
                            result["optimized_items"].append(
                                f"åˆ é™¤é‡å¤å‘é‡æ–‡ä»¶: {duplicate.name}"
                            )

            # å‹ç¼©æ—§çš„è°ƒè¯•æ•°æ®
            debug_dir = self.data_dir / "debug_data"
            if debug_dir.exists():
                old_files = []
                cutoff_time = time.time() - (7 * 24 * 3600)  # 7å¤©å‰

                for debug_file in debug_dir.rglob("*.json"):
                    if debug_file.stat().st_mtime < cutoff_time:
                        old_files.append(debug_file)

                if old_files:
                    # åˆ›å»ºå‹ç¼©æ–‡ä»¶
                    archive_name = (
                        debug_dir
                        / f"old_debug_data_{datetime.now().strftime('%Y%m%d')}.zip"
                    )

                    with zipfile.ZipFile(archive_name, "w", zipfile.ZIP_DEFLATED) as zf:
                        for old_file in old_files:
                            zf.write(old_file, old_file.relative_to(debug_dir))
                            result["space_saved"] += old_file.stat().st_size
                            old_file.unlink()

                    result["optimized_items"].append(
                        f"å‹ç¼©æ—§è°ƒè¯•æ–‡ä»¶: {len(old_files)} ä¸ªæ–‡ä»¶ -> {archive_name.name}"
                    )

            if result["optimized_items"]:
                saved_mb = round(result["space_saved"] / (1024**2), 2)
                self.log(f"æ•°æ®åº“ä¼˜åŒ–å®Œæˆ: èŠ‚çœç©ºé—´ {saved_mb}MB")
            else:
                self.log("æ•°æ®åº“æ— éœ€ä¼˜åŒ–")

        except Exception as e:
            result["error"] = str(e)
            self.log(f"æ•°æ®åº“ä¼˜åŒ–å¤±è´¥: {e}", "ERROR")

        return result

    def generate_maintenance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š"""
        self.log("ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š...")

        report = {
            "timestamp": datetime.now().isoformat(),
            "system_status": self.get_system_status(),
            "maintenance_actions": [],
            "recommendations": [],
        }

        try:
            # æ‰§è¡Œç»´æŠ¤æ“ä½œ
            self.log("æ‰§è¡Œç»´æŠ¤æ“ä½œ...")

            # æ¸…ç†æ—¥å¿—
            log_cleanup = self.clean_logs(keep_days=7)
            if log_cleanup.get("cleaned_files"):
                report["maintenance_actions"].append(
                    {"action": "æ—¥å¿—æ¸…ç†", "result": log_cleanup}
                )

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_cleanup = self.clean_temp_files()
            if temp_cleanup.get("cleaned_files"):
                report["maintenance_actions"].append(
                    {"action": "ä¸´æ—¶æ–‡ä»¶æ¸…ç†", "result": temp_cleanup}
                )

            # æ£€æŸ¥ç£ç›˜ç©ºé—´
            disk_check = self.check_disk_space()
            if disk_check.get("warnings"):
                report["maintenance_actions"].append(
                    {"action": "ç£ç›˜ç©ºé—´æ£€æŸ¥", "result": disk_check}
                )
                report["recommendations"].extend(disk_check.get("recommendations", []))

            # ä¼˜åŒ–æ•°æ®åº“
            db_optimization = self.optimize_databases()
            if db_optimization.get("optimized_items"):
                report["maintenance_actions"].append(
                    {"action": "æ•°æ®åº“ä¼˜åŒ–", "result": db_optimization}
                )

            # ç”Ÿæˆé€šç”¨å»ºè®®
            status = report["system_status"]
            if status.get("file_counts", {}).get("pdf_reports", 0) == 0:
                report["recommendations"].append("ä¸Šä¼ PDFæŠ¥å‘Šæ–‡ä»¶ä»¥ä½¿ç”¨å®Œæ•´åŠŸèƒ½")

            if status.get("log_size", {}).get("total_mb", 0) > 100:
                report["recommendations"].append("æ—¥å¿—æ–‡ä»¶è¾ƒå¤§ï¼Œè€ƒè™‘æ›´é¢‘ç¹çš„æ¸…ç†")

            # ä¿å­˜æŠ¥å‘Š
            report_file = (
                self.logs_dir
                / f"maintenance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            with open(report_file, "w", encoding="utf-8") as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            self.log(f"ç»´æŠ¤æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

        except Exception as e:
            report["error"] = str(e)
            self.log(f"ç»´æŠ¤æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}", "ERROR")

        return report


def main():
    """ä¸»å‡½æ•°"""
    maintainer = SystemMaintainer()

    if len(sys.argv) < 2:
        print("ğŸ”§ æŠ•ç ”RAGç³»ç»Ÿç»´æŠ¤å·¥å…·")
        print("=" * 40)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python system_maintenance.py <command>")
        print()
        print("å¯ç”¨å‘½ä»¤:")
        print("  status       - æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€")
        print("  clean-logs   - æ¸…ç†æ—¥å¿—æ–‡ä»¶")
        print("  clean-temp   - æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
        print("  backup       - å¤‡ä»½æ•°æ®")
        print("  restore <file> - æ¢å¤å¤‡ä»½")
        print("  check-disk   - æ£€æŸ¥ç£ç›˜ç©ºé—´")
        print("  optimize     - ä¼˜åŒ–æ•°æ®åº“")
        print("  report       - ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š")
        print("  full         - æ‰§è¡Œå®Œæ•´ç»´æŠ¤")
        return

    command = sys.argv[1].lower()

    if command == "status":
        status = maintainer.get_system_status()
        print("ç³»ç»ŸçŠ¶æ€:")
        print(json.dumps(status, ensure_ascii=False, indent=2))

    elif command == "clean-logs":
        days = int(sys.argv[2]) if len(sys.argv) > 2 else 7
        result = maintainer.clean_logs(keep_days=days)
        print(f"æ—¥å¿—æ¸…ç†ç»“æœ: {result}")

    elif command == "clean-temp":
        result = maintainer.clean_temp_files()
        print(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†ç»“æœ: {result}")

    elif command == "backup":
        include_logs = "--include-logs" in sys.argv
        result = maintainer.backup_data(include_logs=include_logs)
        print(f"å¤‡ä»½ç»“æœ: {result}")

    elif command == "restore":
        if len(sys.argv) < 3:
            print("è¯·æä¾›å¤‡ä»½æ–‡ä»¶è·¯å¾„")
            sys.exit(1)

        backup_file = sys.argv[2]
        result = maintainer.restore_backup(backup_file)
        print(f"æ¢å¤ç»“æœ: {result}")

    elif command == "check-disk":
        threshold = float(sys.argv[2]) if len(sys.argv) > 2 else 80.0
        result = maintainer.check_disk_space(warn_threshold=threshold)
        print(f"ç£ç›˜æ£€æŸ¥ç»“æœ: {result}")

    elif command == "optimize":
        result = maintainer.optimize_databases()
        print(f"æ•°æ®åº“ä¼˜åŒ–ç»“æœ: {result}")

    elif command == "report":
        report = maintainer.generate_maintenance_report()
        print("ç»´æŠ¤æŠ¥å‘Šå·²ç”Ÿæˆ")

    elif command == "full":
        print("ğŸ”§ æ‰§è¡Œå®Œæ•´ç³»ç»Ÿç»´æŠ¤...")
        report = maintainer.generate_maintenance_report()

        print("\nğŸ“Š ç»´æŠ¤æ‘˜è¦:")
        if report.get("maintenance_actions"):
            for action in report["maintenance_actions"]:
                print(f"  âœ… {action['action']}")

        if report.get("recommendations"):
            print("\nğŸ’¡ å»ºè®®:")
            for rec in report["recommendations"]:
                print(f"  - {rec}")

        print("\nâœ… å®Œæ•´ç»´æŠ¤å®Œæˆ")

    else:
        print(f"æœªçŸ¥å‘½ä»¤: {command}")
        sys.exit(1)


if __name__ == "__main__":
    main()
