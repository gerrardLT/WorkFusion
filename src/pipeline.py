"""
æŠ•ç ”RAGç³»ç»Ÿå®Œæ•´æµæ°´çº¿
æ•´åˆPDFè§£æã€å‘é‡åŒ–ã€æ£€ç´¢ã€é—®ç­”çš„å®Œæ•´æµç¨‹
"""

import json
import logging
import time
from pathlib import Path
from typing import Dict, Any, List, Optional
from dataclasses import dataclass

# ç›´æ¥å¯¼å…¥config.pyæ–‡ä»¶ä¸­çš„get_settings
from config import get_settings
from pdf_parsing_mineru import PDFParsingPipeline
from ingestion import IngestionPipeline
from api_requests import APIProcessor

logger = logging.getLogger(__name__)


@dataclass
class PipelineConfig:
    """æµæ°´çº¿é…ç½®ç±»"""

    def __init__(
        self,
        root_path: Path,
        subset_name: str = "subset.csv",
        questions_file_name: str = "questions.json",
        pdf_reports_dir_name: str = "pdf_reports",
    ):
        self.root_path = root_path
        self.subset_path = root_path / subset_name
        self.questions_file_path = root_path / questions_file_name
        self.pdf_reports_dir = root_path / pdf_reports_dir_name

        # æ•°æ®åº“å’Œè°ƒè¯•ç›®å½•
        self.databases_path = root_path / "databases"
        self.debug_data_path = root_path / "debug_data"

        # å­ç›®å½•
        self.vector_db_dir = self.databases_path / "vector_dbs"
        self.bm25_db_dir = self.databases_path / "bm25"
        self.parsed_reports_path = self.debug_data_path / "parsed_reports"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for path in [
            self.databases_path,
            self.debug_data_path,
            self.vector_db_dir,
            self.bm25_db_dir,
            self.parsed_reports_path,
        ]:
            path.mkdir(parents=True, exist_ok=True)


@dataclass
class RunConfig:
    """è¿è¡Œé…ç½®ç±»"""

    use_vector_dbs: bool = True
    use_bm25_db: bool = True
    top_n_retrieval: int = 10
    api_provider: str = "dashscope"
    llm_model: str = "qwen-turbo-latest"
    embedding_model: str = "text-embedding-v1"
    parallel_requests: int = 1
    enable_reranking: bool = False
    max_context_length: int = 8000


class Pipeline:
    """å¤šåœºæ™¯RAGç³»ç»Ÿä¸»æµæ°´çº¿"""

    def __init__(
        self,
        root_path: Path,
        subset_name: str = "subset.csv",
        questions_file_name: str = "questions.json",
        pdf_reports_dir_name: str = "pdf_reports",
        run_config: RunConfig = None,
        scenario_id: str = "investment",
        tenant_id: str = "default"  # æ–°å¢ï¼šç§Ÿæˆ·IDå‚æ•°
    ):
        """åˆå§‹åŒ–æµæ°´çº¿

        Args:
            root_path: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            subset_name: å…ƒæ•°æ®æ–‡ä»¶å
            questions_file_name: é—®é¢˜æ–‡ä»¶å
            pdf_reports_dir_name: PDFæŠ¥å‘Šç›®å½•å
            run_config: è¿è¡Œé…ç½®
            scenario_id: ä¸šåŠ¡åœºæ™¯ID
            tenant_id: ç§Ÿæˆ·IDï¼ˆç”¨äºæ•°æ®éš”ç¦»ï¼‰
        """
        self.run_config = run_config or RunConfig()
        self.scenario_id = scenario_id
        self.tenant_id = tenant_id  # æ–°å¢ï¼šå­˜å‚¨ç§Ÿæˆ·ID

        # åˆ›å»ºç§Ÿæˆ·çº§é…ç½®ï¼ˆä¿®æ”¹è·¯å¾„ç»“æ„ï¼‰
        self.paths = PipelineConfig(
            root_path, subset_name, questions_file_name, pdf_reports_dir_name
        )

        # é‡å†™å…³é”®è·¯å¾„ä»¥æ”¯æŒç§Ÿæˆ·éš”ç¦»
        self._setup_tenant_paths()

        self.settings = get_settings()

        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼ˆä¼ é€’ç§Ÿæˆ·ä¿¡æ¯ï¼‰
        self.pdf_parser = PDFParsingPipeline(scenario_id=scenario_id)
        self.ingestion = IngestionPipeline(
            api_provider=self.run_config.api_provider,
            tenant_id=tenant_id  # æ–°å¢ï¼šä¼ é€’ç§Ÿæˆ·IDåˆ°Ingestion
        )
        self.api_processor = APIProcessor(provider=self.run_config.api_provider)

        # çŠ¶æ€è·Ÿè¸ª
        self.is_ready = False
        self.databases_loaded = False

        logger.info(f"Pipeline initialized for tenant: {tenant_id}, scenario: {scenario_id}, path: {root_path}")

    def _setup_tenant_paths(self):
        """è®¾ç½®ç§Ÿæˆ·çº§è·¯å¾„ç»“æ„

        å°†åŸæœ‰çš„è·¯å¾„ç»“æ„ä¿®æ”¹ä¸ºç§Ÿæˆ·éš”ç¦»çš„è·¯å¾„ï¼š
        - vector_dbs/{tenant_id}/{scenario_id}/
        - bm25/{tenant_id}/{scenario_id}/
        """
        # é‡å†™å‘é‡æ•°æ®åº“è·¯å¾„ï¼ˆç§Ÿæˆ·çº§éš”ç¦»ï¼‰
        self.paths.vector_db_dir = self.paths.databases_path / "vector_dbs" / self.tenant_id / self.scenario_id

        # é‡å†™BM25æ•°æ®åº“è·¯å¾„ï¼ˆç§Ÿæˆ·çº§éš”ç¦»ï¼‰
        self.paths.bm25_db_dir = self.paths.databases_path / "bm25" / self.tenant_id / self.scenario_id

        # ç¡®ä¿ç§Ÿæˆ·çº§ç›®å½•å­˜åœ¨
        self.paths.vector_db_dir.mkdir(parents=True, exist_ok=True)
        self.paths.bm25_db_dir.mkdir(parents=True, exist_ok=True)

        logger.info(f"Tenant paths setup:")
        logger.info(f"  Vector DB: {self.paths.vector_db_dir}")
        logger.info(f"  BM25 DB: {self.paths.bm25_db_dir}")

    def prepare_documents(self, force_rebuild: bool = False) -> Dict[str, Any]:
        """å‡†å¤‡æ–‡æ¡£ï¼šPDFè§£æ + å‘é‡åŒ–

        Args:
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡æ–°æ„å»º

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        logger.info("å¼€å§‹æ–‡æ¡£å‡†å¤‡æµç¨‹...")

        results = {
            "parsing_results": None,
            "ingestion_results": None,
            "success": True,
            "total_time": 0,
        }

        start_time = time.time()

        try:
            # æ£€æŸ¥PDFæ–‡ä»¶
            pdf_files = list(self.paths.pdf_reports_dir.glob("*.pdf"))
            if not pdf_files:
                logger.warning(f"æœªæ‰¾åˆ°PDFæ–‡ä»¶: {self.paths.pdf_reports_dir}")
                results["success"] = False
                results["error"] = "æœªæ‰¾åˆ°PDFæ–‡ä»¶"
                return results

            logger.info(f"å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°è§£æ
            parsed_files = list(self.paths.parsed_reports_path.glob("*_parsed.json"))
            if not force_rebuild and len(parsed_files) >= len(pdf_files):
                logger.info("å‘ç°å·²è§£ææ–‡ä»¶ï¼Œè·³è¿‡PDFè§£æ")
            else:
                # PDFè§£æ
                logger.info("å¼€å§‹PDFè§£æ...")
                parsing_results = self.pdf_parser.batch_parse_pdfs(
                    str(self.paths.pdf_reports_dir), str(self.paths.parsed_reports_path)
                )
                results["parsing_results"] = parsing_results
                logger.info(
                    f"PDFè§£æå®Œæˆ: {parsing_results['successful']}/{parsing_results['total_files']}"
                )

            # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°æ„å»ºç´¢å¼•
            vector_files = list(self.paths.vector_db_dir.glob("*.faiss"))
            bm25_files = list(self.paths.bm25_db_dir.glob("*.pkl"))

            if not force_rebuild and vector_files and bm25_files:
                logger.info("å‘ç°å·²æ„å»ºç´¢å¼•ï¼Œè·³è¿‡æ•°æ®æ‘„å–")
            else:
                # æ•°æ®æ‘„å–ï¼ˆå‘é‡åŒ–å’ŒBM25ç´¢å¼•ï¼‰
                logger.info("å¼€å§‹æ•°æ®æ‘„å–...")
                ingestion_results = self.ingestion.process_reports(
                    self.paths.parsed_reports_path,
                    self.paths.databases_path,
                    include_bm25=self.run_config.use_bm25_db,
                    include_vector=self.run_config.use_vector_dbs,
                )
                results["ingestion_results"] = ingestion_results
                logger.info(f"æ•°æ®æ‘„å–å®Œæˆ: æˆåŠŸ={ingestion_results['success']}")

            # æ ‡è®°å‡†å¤‡å®Œæˆ
            self.is_ready = True

            # è®¡ç®—æ€»æ—¶é—´
            results["total_time"] = time.time() - start_time

            logger.info(f"æ–‡æ¡£å‡†å¤‡å®Œæˆï¼Œæ€»è€—æ—¶: {results['total_time']:.2f}ç§’")

            return results

        except Exception as e:
            logger.error(f"æ–‡æ¡£å‡†å¤‡å¤±è´¥: {str(e)}")
            results["success"] = False
            results["error"] = str(e)
            results["total_time"] = time.time() - start_time
            return results

    def load_databases(self) -> bool:
        """åŠ è½½æ•°æ®åº“ç´¢å¼•åˆ°å†…å­˜"""
        try:
            # è¿™é‡Œå¯ä»¥é¢„åŠ è½½FAISSç´¢å¼•ç­‰
            # æš‚æ—¶ç®€åŒ–ï¼Œæ ‡è®°ä¸ºå·²åŠ è½½
            self.databases_loaded = True
            logger.info("æ•°æ®åº“åŠ è½½å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"æ•°æ®åº“åŠ è½½å¤±è´¥: {str(e)}")
            return False

    def answer_question(
        self,
        question: str,
        company: Optional[str] = None,
        question_type: str = "string",
    ) -> Dict[str, Any]:
        """å›ç­”å•ä¸ªé—®é¢˜ï¼ˆä½¿ç”¨ Agentic RAGï¼‰

        Args:
            question: é—®é¢˜æ–‡æœ¬
            company: ç›®æ ‡å…¬å¸ï¼ˆå¯é€‰ï¼‰
            question_type: é—®é¢˜ç±»å‹

        Returns:
            å›ç­”ç»“æœ
        """
        if not self.is_ready:
            return {
                "success": False,
                "error": "Pipelineæœªå‡†å¤‡å°±ç»ªï¼Œè¯·å…ˆè°ƒç”¨prepare_documents()",
            }

        start_time = time.time()

        try:
            logger.info(f"ğŸ¤– Pipelineå¤„ç†é—®é¢˜: {question}")

            # âœ… ä½¿ç”¨ QuestionsProcessorï¼ˆé›†æˆäº†å®Œæ•´çš„ Agentic RAGï¼‰
            try:
                from questions_processing import QuestionsProcessor

                processor = QuestionsProcessor(
                    api_provider=self.run_config.api_provider,
                    scenario_id=self.scenario_id,
                    tenant_id=self.tenant_id  # æ–°å¢ï¼šä¼ é€’ç§Ÿæˆ·ID
                )

                # è°ƒç”¨å®Œæ•´çš„é—®é¢˜å¤„ç†æµç¨‹
                result = processor.process_question(
                    question=question,
                    company=company,
                    question_type=question_type
                )

                logger.info(f"âœ… QuestionsProcessorå¤„ç†å®Œæˆ: success={result.get('success')}")

                return result

            except Exception as e:
                logger.error(f"QuestionsProcessorè°ƒç”¨å¤±è´¥: {e}ï¼Œé™çº§ä¸ºç®€å•LLM")

                # é™çº§æ–¹æ¡ˆï¼šç®€å•LLMå›ç­”
                system_prompt_map = {
                    "investment": "ä½ æ˜¯ä¸“ä¸šçš„æŠ•èµ„åˆ†æå¸ˆï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·å…³äºæŠ•èµ„å’Œå…¬å¸åˆ†æçš„é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®ã€å®¢è§‚çš„åˆ†æå’Œå»ºè®®ã€‚",
                    "tender": "ä½ æ˜¯ä¸“ä¸šçš„æ‹›æŠ•æ ‡åˆ†æå¸ˆï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·å…³äºæ‹›æŠ•æ ‡çš„é—®é¢˜ã€‚è¯·æä¾›å‡†ç¡®çš„æ‹›æŠ•æ ‡ä¿¡æ¯è§£è¯»å’Œä¸“ä¸šå»ºè®®ã€‚",
                    "enterprise": "ä½ æ˜¯ä¸“ä¸šçš„ä¼ä¸šç®¡ç†é¡¾é—®ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·å…³äºä¼ä¸šç®¡ç†çš„é—®é¢˜ã€‚è¯·æä¾›ä¸“ä¸šçš„å»ºè®®ã€‚"
                }
                system_prompt = system_prompt_map.get(self.scenario_id, "ä½ æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºä½ çš„çŸ¥è¯†å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")

                response = self.api_processor.send_message(
                    system_content=system_prompt,
                human_content=question,
                temperature=0.3,
                max_tokens=500,
            )

                processing_time = time.time() - start_time

                return {
                    "success": True,
                    "answer": response,
                    "question": question,
                    "question_type": question_type,
                    "company": company,
                    "processing_time": processing_time,
                    "reasoning": "Agentic RAGä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•LLMå›ç­”",
                    "relevant_pages": [],
                    "confidence": 0.5,
                    "agentic_rag_enabled": False
                }

        except Exception as e:
            logger.error(f"é—®é¢˜å›ç­”å¤±è´¥: {str(e)}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "question": question,
                "processing_time": time.time() - start_time,
                "confidence": 0.1
            }

    def batch_answer_questions(
        self, questions_file: Optional[Path] = None
    ) -> Dict[str, Any]:
        """æ‰¹é‡å›ç­”é—®é¢˜

        Args:
            questions_file: é—®é¢˜æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„æ–‡ä»¶

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        if questions_file is None:
            questions_file = self.paths.questions_file_path

        if not questions_file.exists():
            return {"success": False, "error": f"é—®é¢˜æ–‡ä»¶ä¸å­˜åœ¨: {questions_file}"}

        try:
            # åŠ è½½é—®é¢˜
            with open(questions_file, "r", encoding="utf-8") as f:
                questions_data = json.load(f)

            logger.info(f"åŠ è½½ {len(questions_data)} ä¸ªé—®é¢˜")

            results = {
                "total_questions": len(questions_data),
                "successful": 0,
                "failed": 0,
                "answers": [],
                "total_time": 0,
            }

            start_time = time.time()

            # é€ä¸ªå¤„ç†é—®é¢˜
            for i, question_item in enumerate(questions_data):
                logger.info(f"å¤„ç†é—®é¢˜ {i+1}/{len(questions_data)}")

                # æå–é—®é¢˜ä¿¡æ¯
                question_text = question_item.get("question_text", "")
                question_type = question_item.get("question_type", "string")
                company = question_item.get("target_companies", [None])[0]

                # å›ç­”é—®é¢˜
                answer_result = self.answer_question(
                    question_text, company, question_type
                )

                # è®°å½•ç»“æœ
                if answer_result["success"]:
                    results["successful"] += 1
                else:
                    results["failed"] += 1

                results["answers"].append(
                    {
                        "question_id": question_item.get("question_id", f"q_{i}"),
                        **answer_result,
                    }
                )

            results["total_time"] = time.time() - start_time
            results["success"] = results["failed"] == 0

            logger.info(
                f"æ‰¹é‡é—®ç­”å®Œæˆ: {results['successful']}/{results['total_questions']} æˆåŠŸ"
            )

            return results

        except Exception as e:
            logger.error(f"æ‰¹é‡é—®ç­”å¤±è´¥: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "total_questions": 0,
                "successful": 0,
                "failed": 0,
            }

    def get_status(self) -> Dict[str, Any]:
        """è·å–æµæ°´çº¿çŠ¶æ€"""
        return {
            "is_ready": self.is_ready,
            "databases_loaded": self.databases_loaded,
            "config": {
                "api_provider": self.run_config.api_provider,
                "llm_model": self.run_config.llm_model,
                "embedding_model": self.run_config.embedding_model,
                "use_vector_dbs": self.run_config.use_vector_dbs,
                "use_bm25_db": self.run_config.use_bm25_db,
            },
            "paths": {
                "pdf_reports": str(self.paths.pdf_reports_dir),
                "databases": str(self.paths.databases_path),
                "questions_file": str(self.paths.questions_file_path),
            },
        }


# ä¾¿æ·å‡½æ•°
def create_pipeline(data_path: str, api_provider: str = "dashscope", scenario_id: str = "investment") -> Pipeline:
    """åˆ›å»ºPipelineå®ä¾‹çš„ä¾¿æ·å‡½æ•°"""
    config = RunConfig(api_provider=api_provider)
    return Pipeline(Path(data_path), run_config=config, scenario_id=scenario_id)


def get_default_pipeline() -> Pipeline:
    """è·å–é»˜è®¤é…ç½®çš„Pipeline"""
    settings = get_settings()
    return create_pipeline(str(settings.data_dir))
