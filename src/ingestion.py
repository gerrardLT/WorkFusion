"""
æ•°æ®æ‘„å–å’Œå‘é‡åŒ–æ¨¡å—
è´Ÿè´£å¤„ç†PDFè§£æç»“æœï¼Œåˆ›å»ºBM25ç´¢å¼•å’Œå‘é‡æ•°æ®åº“
"""

import json
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
import hashlib
from datetime import datetime
import time

import numpy as np
import faiss
from tqdm import tqdm
from rank_bm25 import BM25Okapi
from tenacity import retry, wait_exponential, stop_after_attempt

from config import get_settings
from api_requests import APIProcessor
from data_models import ReportMetadata, ChunkMetadata, ProcessingStatus

logger = logging.getLogger(__name__)


class BM25Ingestor:
    """BM25ç´¢å¼•æ„å»ºä¸ç®¡ç†ç±»"""

    def __init__(self):
        """åˆå§‹åŒ–BM25æ„å»ºå™¨"""
        self.settings = get_settings()
        logger.info("BM25Ingestoråˆå§‹åŒ–å®Œæˆ")

    def create_bm25_index(self, chunks: List[str]) -> BM25Okapi:
        """ä»æ–‡æœ¬å—åˆ—è¡¨åˆ›å»ºBM25ç´¢å¼•

        Args:
            chunks: æ–‡æœ¬å—åˆ—è¡¨

        Returns:
            BM25ç´¢å¼•å¯¹è±¡
        """
        try:
            # ç®€å•çš„ä¸­æ–‡åˆ†è¯ï¼ˆæŒ‰å­—ç¬¦å’Œç©ºæ ¼åˆ†å‰²ï¼‰
            tokenized_chunks = []
            for chunk in chunks:
                # å¯¹ä¸­æ–‡æŒ‰å­—ç¬¦åˆ†å‰²ï¼Œå¯¹è‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²
                tokens = []
                current_word = ""

                for char in chunk:
                    if "\u4e00" <= char <= "\u9fff":  # ä¸­æ–‡å­—ç¬¦
                        if current_word:
                            tokens.append(current_word)
                            current_word = ""
                        tokens.append(char)
                    elif char.isalnum():  # è‹±æ–‡æ•°å­—å­—ç¬¦
                        current_word += char
                    else:  # æ ‡ç‚¹ç¬¦å·ç­‰
                        if current_word:
                            tokens.append(current_word)
                            current_word = ""
                        if not char.isspace():
                            tokens.append(char)

                if current_word:
                    tokens.append(current_word)

                tokenized_chunks.append([t for t in tokens if t.strip()])

            logger.debug(f"åˆ›å»ºBM25ç´¢å¼•ï¼Œæ–‡æ¡£æ•°: {len(tokenized_chunks)}")
            return BM25Okapi(tokenized_chunks)

        except Exception as e:
            logger.error(f"åˆ›å»ºBM25ç´¢å¼•å¤±è´¥: {str(e)}")
            raise

    def process_single_report(
        self, report_data: Dict[str, Any], output_dir: Path
    ) -> Tuple[bool, Optional[str]]:
        """å¤„ç†å•ä¸ªæŠ¥å‘Šï¼Œç”ŸæˆBM25ç´¢å¼•

        Args:
            report_data: è§£æåçš„æŠ¥å‘Šæ•°æ®
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            (æ˜¯å¦æˆåŠŸ, è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æå–æ–‡æœ¬å—
            chunks = []
            if "pages" in report_data:
                for page in report_data["pages"]:
                    if page.get("text"):
                        chunks.append(page["text"])

            # å¦‚æœæœ‰æ•´ä½“æ–‡æœ¬å†…å®¹ï¼Œä¹ŸåŠ å…¥
            if "text_content" in report_data and report_data["text_content"]:
                chunks.append(report_data["text_content"])

            if not chunks:
                error_msg = "æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æœ¬å†…å®¹"
                logger.warning(f"BM25ç´¢å¼•æ„å»ºè·³è¿‡: {error_msg}")
                return False, error_msg

            # åˆ›å»ºBM25ç´¢å¼•
            bm25_index = self.create_bm25_index(chunks)

            # ç”Ÿæˆæ–‡ä»¶ID
            file_id = self._generate_file_id(report_data)

            # ä½¿ç”¨MD5 hashç”Ÿæˆå®‰å…¨æ–‡ä»¶åï¼ˆé¿å…ä¸­æ–‡ï¼‰
            import hashlib
            safe_filename = hashlib.md5(file_id.encode()).hexdigest()

            # ä¿å­˜ç´¢å¼•
            output_dir.mkdir(parents=True, exist_ok=True)
            output_file = output_dir / f"{safe_filename}_bm25.pkl"

            logger.debug(f"åŸå§‹file_id: {file_id}, MD5: {safe_filename}")

            with open(output_file, "wb") as f:
                pickle.dump(
                    {
                        "index": bm25_index,
                        "chunks": chunks,
                        "metadata": {
                            "file_id": file_id,
                            "chunk_count": len(chunks),
                            "created_at": datetime.now().isoformat(),
                            "index_type": "BM25Okapi",
                        },
                    },
                    f,
                )

            logger.info(f"BM25ç´¢å¼•å·²ä¿å­˜: {output_file}")
            return True, str(output_file)

        except Exception as e:
            error_msg = f"BM25ç´¢å¼•æ„å»ºå¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return False, error_msg

    def batch_process_reports(
        self, reports_dir: Path, output_dir: Path
    ) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æŠ¥å‘Šï¼Œç”ŸæˆBM25ç´¢å¼•

        Args:
            reports_dir: æŠ¥å‘Šæ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        report_files = list(reports_dir.glob("*_parsed.json"))

        logger.info(f"å¼€å§‹æ‰¹é‡BM25ç´¢å¼•æ„å»ºï¼Œå…± {len(report_files)} ä¸ªæ–‡ä»¶")

        results = {
            "total_files": len(report_files),
            "successful": 0,
            "failed": 0,
            "results": [],
        }

        for report_file in tqdm(report_files, desc="BM25ç´¢å¼•æ„å»º"):
            try:
                with open(report_file, "r", encoding="utf-8") as f:
                    report_data = json.load(f)

                success, result = self.process_single_report(report_data, output_dir)

                if success:
                    results["successful"] += 1
                else:
                    results["failed"] += 1

                results["results"].append(
                    {
                        "file_path": str(report_file),
                        "success": success,
                        "result": result,
                    }
                )

            except Exception as e:
                results["failed"] += 1
                results["results"].append(
                    {"file_path": str(report_file), "success": False, "result": str(e)}
                )
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {report_file}, é”™è¯¯: {str(e)}")

        logger.info(
            f"BM25æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {results['successful']}, å¤±è´¥ {results['failed']}"
        )
        return results

    def _generate_file_id(self, report_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡ä»¶ID"""
        # ä½¿ç”¨å¤„ç†ä¿¡æ¯ä¸­çš„file_idï¼Œæˆ–ä»æ–‡ä»¶ä¿¡æ¯ç”Ÿæˆ
        if (
            "processing_info" in report_data
            and "file_id" in report_data["processing_info"]
        ):
            return report_data["processing_info"]["file_id"]

        # ä»æ–‡æ¡£ä¿¡æ¯ç”ŸæˆID
        doc_info = report_data.get("document_info", {})
        file_name = doc_info.get("file_name", "unknown")
        file_size = doc_info.get("file_size", 0)

        # ç”Ÿæˆå”¯ä¸€ID
        content = f"{file_name}_{file_size}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return hashlib.md5(content.encode()).hexdigest()


class VectorDBIngestor:
    """å‘é‡æ•°æ®åº“æ„å»ºä¸ç®¡ç†ç±»"""

    def __init__(self, api_provider: str = "dashscope"):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨

        Args:
            api_provider: APIæä¾›å•†åç§°
        """
        self.settings = get_settings()
        self.api_processor = APIProcessor(provider=api_provider)
        # ğŸš€ æ€§èƒ½ä¼˜åŒ–ï¼šå¢åŠ æ‰¹æ¬¡å¤§å°ï¼Œå‡å°‘å»¶è¿Ÿ
        self.batch_size = 25       # ä»5æå‡åˆ°25ï¼Œå‡å°‘APIè°ƒç”¨æ¬¡æ•°
        self.base_delay = 0.1      # ä»0.5så‡å°‘åˆ°0.1s
        self.max_delay = 1.0       # ä»5.0så‡å°‘åˆ°1.0s
        # å•æ–‡ä»¶æ¨¡å¼ä¸‹ä¸éœ€è¦æ–‡ä»¶é—´å»¶è¿Ÿ

        logger.info(f"VectorDBIngestoråˆå§‹åŒ–å®Œæˆï¼Œprovider: {api_provider}, batch_size: {self.batch_size}")

    @retry(
        wait=wait_exponential(multiplier=2, min=2, max=30), stop=stop_after_attempt(5)
    )
    def get_embeddings(
        self, texts: List[str], model: Optional[str] = None
    ) -> List[List[float]]:
        """è·å–æ–‡æœ¬åµŒå…¥å‘é‡ï¼Œæ”¯æŒé‡è¯•å’Œæ‰¹å¤„ç†

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            model: åµŒå…¥æ¨¡å‹åç§°

        Returns:
            åµŒå…¥å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []

        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [text.strip() for text in texts if text.strip()]
        if not valid_texts:
            raise ValueError("æ‰€æœ‰è¾“å…¥æ–‡æœ¬éƒ½ä¸ºç©º")

        try:
            logger.debug(f"è·å–åµŒå…¥å‘é‡ï¼Œæ–‡æœ¬æ•°é‡: {len(valid_texts)}")

            all_embeddings = []

            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(valid_texts), self.batch_size):
                batch = valid_texts[i : i + self.batch_size]

                logger.debug(
                    f"å¤„ç†æ‰¹æ¬¡ {i//self.batch_size + 1}, æ–‡æœ¬æ•°é‡: {len(batch)}"
                )

                # è°ƒç”¨APIè·å–åµŒå…¥å‘é‡
                embeddings = self.api_processor.get_embeddings(batch, model)
                all_embeddings.extend(embeddings)

                # åŠ¨æ€å»¶è¿Ÿï¼Œé¿å…APIé™åˆ¶
                if i + self.batch_size < len(valid_texts):
                    # æ ¹æ®æ‰¹æ¬¡æ•°é‡åŠ¨æ€è°ƒæ•´å»¶è¿Ÿ
                    batch_number = i // self.batch_size + 1
                    delay = min(self.base_delay * batch_number, self.max_delay)
                    logger.debug(f"æ‰¹æ¬¡é—´å»¶è¿Ÿ: {delay:.2f}ç§’")
                    time.sleep(delay)

            logger.info(f"æˆåŠŸè·å– {len(all_embeddings)} ä¸ªåµŒå…¥å‘é‡")
            return all_embeddings

        except Exception as e:
            logger.error(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {str(e)}")
            raise

    def create_vector_index(self, embeddings: List[List[float]]) -> faiss.IndexFlatIP:
        """åˆ›å»ºFAISSå‘é‡ç´¢å¼•

        Args:
            embeddings: åµŒå…¥å‘é‡åˆ—è¡¨

        Returns:
            FAISSç´¢å¼•å¯¹è±¡
        """
        if not embeddings:
            raise ValueError("åµŒå…¥å‘é‡åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            embeddings_array = np.array(embeddings, dtype=np.float32)

            # æ£€æŸ¥ç»´åº¦ä¸€è‡´æ€§
            if len(embeddings_array.shape) != 2:
                raise ValueError("åµŒå…¥å‘é‡æ ¼å¼é”™è¯¯")

            # æ ‡å‡†åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            faiss.normalize_L2(embeddings_array)

            # åˆ›å»ºå†…ç§¯ç´¢å¼•ï¼ˆé€‚ç”¨äºæ ‡å‡†åŒ–åçš„å‘é‡ï¼‰
            dimension = embeddings_array.shape[1]
            index = faiss.IndexFlatIP(dimension)

            # æ·»åŠ å‘é‡åˆ°ç´¢å¼•
            index.add(embeddings_array)

            logger.info(
                f"åˆ›å»ºFAISSç´¢å¼•æˆåŠŸï¼Œç»´åº¦: {dimension}, å‘é‡æ•°é‡: {len(embeddings)}"
            )
            return index

        except Exception as e:
            logger.error(f"åˆ›å»ºFAISSç´¢å¼•å¤±è´¥: {str(e)}")
            raise

    def process_single_report(
        self, report_data: Dict[str, Any], output_dir: Path, model: Optional[str] = None
    ) -> Tuple[bool, Optional[str]]:
        """å¤„ç†å•ä¸ªæŠ¥å‘Šï¼Œç”Ÿæˆå‘é‡æ•°æ®åº“

        Args:
            report_data: è§£æåçš„æŠ¥å‘Šæ•°æ®
            output_dir: è¾“å‡ºç›®å½•
            model: åµŒå…¥æ¨¡å‹åç§°

        Returns:
            (æ˜¯å¦æˆåŠŸ, è¾“å‡ºæ–‡ä»¶è·¯å¾„æˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            # æå–æ–‡æœ¬å—
            chunks = []
            chunk_metadata = []

            if "pages" in report_data:
                for page_idx, page in enumerate(report_data["pages"]):
                    if page.get("text"):
                        chunks.append(page["text"])
                        chunk_metadata.append(
                            {
                                "type": "page_text",
                                "page_number": page.get("page_number", page_idx + 1),
                                "word_count": len(page["text"].split()),
                            }
                        )

            # å¤„ç†è¡¨æ ¼å†…å®¹
            if "tables" in report_data:
                for table_idx, table in enumerate(report_data["tables"]):
                    # å°†è¡¨æ ¼è½¬æ¢ä¸ºæ–‡æœ¬
                    if "data" in table and table["data"]:
                        table_text = self._table_to_text(table["data"])
                        chunks.append(table_text)
                        chunk_metadata.append(
                            {
                                "type": "table",
                                "table_id": table.get("table_id", f"table_{table_idx}"),
                                "page_number": table.get("page_number"),
                                "word_count": len(table_text.split()),
                            }
                        )

            if not chunks:
                error_msg = "æŠ¥å‘Šä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ–‡æœ¬å†…å®¹"
                logger.warning(f"å‘é‡åŒ–è·³è¿‡: {error_msg}")
                return False, error_msg

            # è·å–åµŒå…¥å‘é‡
            embeddings = self.get_embeddings(chunks, model)

            # åˆ›å»ºå‘é‡ç´¢å¼•
            vector_index = self.create_vector_index(embeddings)

            # ç”Ÿæˆæ–‡ä»¶ID
            file_id = self._generate_file_id(report_data)

            # ä¿å­˜ç»“æœ
            output_dir.mkdir(parents=True, exist_ok=True)

            # ä¿å­˜FAISSç´¢å¼• - å½»åº•ä½¿ç”¨MD5æ–‡ä»¶åé¿å…ä¸­æ–‡è·¯å¾„é—®é¢˜
            import hashlib

            # ä½¿ç”¨MD5 hashç”Ÿæˆçº¯è‹±æ–‡å®‰å…¨æ–‡ä»¶å
            safe_filename = hashlib.md5(file_id.encode()).hexdigest()

            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç›´æ¥ä½¿ç”¨MD5æ–‡ä»¶åä¿å­˜ï¼ˆæ— éœ€ä¸´æ—¶ç›®å½•ï¼‰
            faiss_file = output_dir / f"{safe_filename}_vector.faiss"

            try:
                logger.debug(f"ä¿å­˜FAISSç´¢å¼•åˆ°: {faiss_file}")
                logger.debug(f"åŸå§‹file_id: {file_id}, MD5: {safe_filename}")

                # ç›´æ¥å†™å…¥FAISSæ–‡ä»¶ï¼ˆæ–‡ä»¶åå·²æ˜¯çº¯è‹±æ–‡ï¼‰
                faiss.write_index(vector_index, str(faiss_file))
                logger.info(f"âœ… FAISSç´¢å¼•ä¿å­˜æˆåŠŸ: {faiss_file.name}")

            except Exception as faiss_error:
                logger.error(f"FAISSæ–‡ä»¶ä¿å­˜å¤±è´¥: {faiss_error}")
                raise faiss_error

            # ä¿å­˜å—ä¿¡æ¯å’Œå…ƒæ•°æ®ï¼ˆä¹Ÿä½¿ç”¨MD5æ–‡ä»¶åä¿æŒä¸€è‡´ï¼‰
            metadata_file = output_dir / f"{safe_filename}_chunks.json"
            with open(metadata_file, "w", encoding="utf-8") as f:
                json.dump(
                    {
                        "file_id": file_id,
                        "chunks": chunks,
                        "chunk_metadata": chunk_metadata,
                        "vector_info": {
                            "dimension": vector_index.d,
                            "total_vectors": vector_index.ntotal,
                            "index_type": "FlatIP",
                        },
                        "processing_info": {
                            "model": model or self.settings.dashscope.embedding_model,
                            "created_at": datetime.now().isoformat(),
                            "batch_size": self.batch_size,
                        },
                    },
                    f,
                    ensure_ascii=False,
                    indent=2,
                )

            logger.info(f"å‘é‡åŒ–å®Œæˆ: {faiss_file}")
            return True, str(faiss_file)

        except Exception as e:
            error_msg = f"å‘é‡åŒ–å¤„ç†å¤±è´¥: {str(e)}"
            logger.error(error_msg)
            return False, error_msg

    def batch_process_reports(
        self, reports_dir: Path, output_dir: Path, model: Optional[str] = None
    ) -> Dict[str, Any]:
        """æ‰¹é‡å¤„ç†æŠ¥å‘Šï¼Œç”Ÿæˆå‘é‡æ•°æ®åº“

        Args:
            reports_dir: æŠ¥å‘Šæ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            model: åµŒå…¥æ¨¡å‹åç§°

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        report_files = list(reports_dir.glob("*_parsed.json"))

        logger.info(f"å¼€å§‹æ‰¹é‡å‘é‡åŒ–ï¼Œå…± {len(report_files)} ä¸ªæ–‡ä»¶")

        results = {
            "total_files": len(report_files),
            "successful": 0,
            "failed": 0,
            "results": [],
        }

        for idx, report_file in enumerate(tqdm(report_files, desc="å‘é‡åŒ–å¤„ç†"), 1):
            try:
                with open(report_file, "r", encoding="utf-8") as f:
                    report_data = json.load(f)

                logger.info(f"å¤„ç†æ–‡ä»¶ {idx}/{len(report_files)}: {report_file.name}")

                success, result = self.process_single_report(
                    report_data, output_dir, model
                )

                if success:
                    results["successful"] += 1
                    logger.info(f"âœ… æ–‡ä»¶ {idx} å‘é‡åŒ–æˆåŠŸ")
                else:
                    results["failed"] += 1
                    logger.warning(f"âŒ æ–‡ä»¶ {idx} å‘é‡åŒ–å¤±è´¥: {result}")

                results["results"].append(
                    {
                        "file_path": str(report_file),
                        "success": success,
                        "result": result,
                    }
                )

                # æ–‡ä»¶é—´å»¶è¿Ÿï¼Œé¿å…APIè°ƒç”¨è¿‡äºå¯†é›†
                if idx < len(report_files):
                    file_delay = min(2.0 + (idx * 0.5), 10.0)  # æ–‡ä»¶é—´å»¶è¿Ÿé€æ¸å¢åŠ 
                    logger.debug(f"æ–‡ä»¶é—´å»¶è¿Ÿ: {file_delay:.2f}ç§’")
                    time.sleep(file_delay)

            except Exception as e:
                results["failed"] += 1
                results["results"].append(
                    {"file_path": str(report_file), "success": False, "result": str(e)}
                )
                logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥: {report_file}, é”™è¯¯: {str(e)}")

                # å‡ºé”™æ—¶ä¹Ÿè¦å»¶è¿Ÿï¼Œé¿å…è¿ç»­å¤±è´¥
                time.sleep(2.0)

        logger.info(
            f"å‘é‡åŒ–æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {results['successful']}, å¤±è´¥ {results['failed']}"
        )
        return results

    def _table_to_text(self, table_data: List[List[Any]]) -> str:
        """å°†è¡¨æ ¼æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬"""
        if not table_data:
            return ""

        text_rows = []
        for row in table_data:
            # å°†æ¯è¡Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶è¿æ¥
            row_text = " | ".join([str(cell) for cell in row if cell is not None])
            text_rows.append(row_text)

        return "\n".join(text_rows)

    def _generate_file_id(self, report_data: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ–‡ä»¶ID"""
        # ä½¿ç”¨å¤„ç†ä¿¡æ¯ä¸­çš„file_idï¼Œæˆ–ä»æ–‡ä»¶ä¿¡æ¯ç”Ÿæˆ
        if (
            "processing_info" in report_data
            and "file_id" in report_data["processing_info"]
        ):
            return report_data["processing_info"]["file_id"]

        # ä»æ–‡æ¡£ä¿¡æ¯ç”ŸæˆID
        doc_info = report_data.get("document_info", {})
        file_name = doc_info.get("file_name", "unknown")
        file_size = doc_info.get("file_size", 0)

        # ç”Ÿæˆå”¯ä¸€ID
        content = f"{file_name}_{file_size}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return hashlib.md5(content.encode()).hexdigest()


class IngestionPipeline:
    """æ•°æ®æ‘„å–æµæ°´çº¿ï¼Œæ•´åˆBM25å’Œå‘é‡æ•°æ®åº“æ„å»º"""

    def __init__(self, api_provider: str = "dashscope", tenant_id: str = "default"):
        """åˆå§‹åŒ–æ•°æ®æ‘„å–æµæ°´çº¿

        Args:
            api_provider: APIæä¾›å•†åç§°
            tenant_id: ç§Ÿæˆ·IDï¼ˆç”¨äºæ•°æ®éš”ç¦»ï¼‰
        """
        self.settings = get_settings()
        self.tenant_id = tenant_id  # æ–°å¢ï¼šå­˜å‚¨ç§Ÿæˆ·ID
        self.bm25_ingestor = BM25Ingestor()
        self.vector_ingestor = VectorDBIngestor(api_provider)

        logger.info(f"æ•°æ®æ‘„å–æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ (ç§Ÿæˆ·: {tenant_id})")

    def process_reports(
        self,
        reports_dir: Path,
        output_dir: Optional[Path] = None,
        model: Optional[str] = None,
        include_bm25: bool = True,
        include_vector: bool = True,
    ) -> Dict[str, Any]:
        """å®Œæ•´çš„æŠ¥å‘Šå¤„ç†æµç¨‹

        Args:
            reports_dir: è§£æåçš„æŠ¥å‘Šç›®å½•
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºæ•°æ®åº“ç›®å½•
            model: åµŒå…¥æ¨¡å‹åç§°
            include_bm25: æ˜¯å¦æ„å»ºBM25ç´¢å¼•
            include_vector: æ˜¯å¦æ„å»ºå‘é‡æ•°æ®åº“

        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        if output_dir is None:
            output_dir = self.settings.db_dir

        output_dir = Path(output_dir)

        logger.info(f"å¼€å§‹å®Œæ•´æ•°æ®æ‘„å–æµç¨‹")
        logger.info(f"è¾“å…¥ç›®å½•: {reports_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"BM25ç´¢å¼•: {include_bm25}, å‘é‡æ•°æ®åº“: {include_vector}")

        results = {
            "input_dir": str(reports_dir),
            "output_dir": str(output_dir),
            "bm25_results": None,
            "vector_results": None,
            "total_time": 0,
            "success": True,
        }

        start_time = time.time()

        try:
            # BM25ç´¢å¼•æ„å»º
            if include_bm25:
                logger.info("å¼€å§‹BM25ç´¢å¼•æ„å»º...")
                bm25_output = output_dir / "bm25"
                bm25_results = self.bm25_ingestor.batch_process_reports(
                    reports_dir, bm25_output
                )
                results["bm25_results"] = bm25_results
                logger.info(
                    f"BM25ç´¢å¼•æ„å»ºå®Œæˆ: {bm25_results['successful']}/{bm25_results['total_files']}"
                )

            # å‘é‡æ•°æ®åº“æ„å»º
            if include_vector:
                logger.info("å¼€å§‹å‘é‡æ•°æ®åº“æ„å»º...")
                vector_output = output_dir / "vector_dbs"
                vector_results = self.vector_ingestor.batch_process_reports(
                    reports_dir, vector_output, model
                )
                results["vector_results"] = vector_results
                logger.info(
                    f"å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ: {vector_results['successful']}/{vector_results['total_files']}"
                )

            # è®¡ç®—æ€»æ—¶é—´
            end_time = time.time()
            results["total_time"] = end_time - start_time

            # åˆ¤æ–­æ•´ä½“æˆåŠŸçŠ¶æ€
            bm25_success = not include_bm25 or (
                results["bm25_results"] and results["bm25_results"]["failed"] == 0
            )
            vector_success = not include_vector or (
                results["vector_results"] and results["vector_results"]["failed"] == 0
            )

            results["success"] = bm25_success and vector_success

            logger.info(f"æ•°æ®æ‘„å–æµæ°´çº¿å®Œæˆï¼Œæ€»è€—æ—¶: {results['total_time']:.2f}ç§’")

            return results

        except Exception as e:
            logger.error(f"æ•°æ®æ‘„å–æµæ°´çº¿å¤±è´¥: {str(e)}")
            results["success"] = False
            results["error"] = str(e)
            results["total_time"] = time.time() - start_time
            return results


# ä¾¿æ·å‡½æ•°
def create_bm25_index(chunks: List[str]) -> BM25Okapi:
    """åˆ›å»ºBM25ç´¢å¼•çš„ä¾¿æ·å‡½æ•°"""
    ingestor = BM25Ingestor()
    return ingestor.create_bm25_index(chunks)


def get_embeddings(
    texts: List[str], api_provider: str = "dashscope"
) -> List[List[float]]:
    """è·å–åµŒå…¥å‘é‡çš„ä¾¿æ·å‡½æ•°"""
    ingestor = VectorDBIngestor(api_provider)
    return ingestor.get_embeddings(texts)


def process_reports_pipeline(
    reports_dir: Union[str, Path],
    output_dir: Optional[Union[str, Path]] = None,
    **kwargs,
) -> Dict[str, Any]:
    """å®Œæ•´æŠ¥å‘Šå¤„ç†æµç¨‹çš„ä¾¿æ·å‡½æ•°"""
    pipeline = IngestionPipeline()
    return pipeline.process_reports(
        Path(reports_dir), Path(output_dir) if output_dir else None, **kwargs
    )
