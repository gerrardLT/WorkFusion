"""
åŸºäºMinerUçš„PDFè§£ææ¨¡å—
æ›¿æ¢åŸé¡¹ç›®çš„Doclingè§£æï¼Œæä¾›æ›´å¥½çš„ä¸­æ–‡æ”¯æŒ
æ”¯æŒå‘½ä»¤è¡Œå¹¶è¡Œè§£æä¼˜åŒ–
"""

import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, Tuple, List
import tempfile
from datetime import datetime
import time
import os

import subprocess
import shutil

# æ£€æŸ¥ mineru å‘½ä»¤è¡Œå·¥å…·æ˜¯å¦å¯ç”¨
MINERU_CLI_AVAILABLE = shutil.which("mineru") is not None

import fitz  # PyMuPDF
import pdfplumber

from config import get_settings

logger = logging.getLogger(__name__)


class MinerUPDFParser:
    """åŸºäºMinerUçš„PDFè§£æå™¨ï¼Œæ”¯æŒå‘½ä»¤è¡Œå¹¶è¡Œè§£æä¼˜åŒ–"""

    def __init__(self, scenario_id: str = "investment", progress_callback=None):
        self.settings = get_settings()
        self.scenario_id = scenario_id
        self.progress_callback = progress_callback  # è¿›åº¦å›è°ƒå‡½æ•°

    def parse_pdf_with_mineru(self, pdf_path: str) -> Dict[str, Any]:
        """æ™ºèƒ½è§£æPDFæ–‡æ¡£ï¼ˆä¼˜å…ˆä½¿ç”¨ PyMuPDF + pdfplumberï¼Œéœ€è¦æ—¶æ‰ç”¨ MinerU OCRï¼‰

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            è§£æç»“æœå­—å…¸
        """
        # è·å–é¡µæ•°
        doc = fitz.open(pdf_path)
        total_pages = len(doc)
        doc.close()

        logger.info(f"ğŸ“„ PDF æ–‡ä»¶: {Path(pdf_path).name}ï¼Œæ€»é¡µæ•°: {total_pages}")

        # æŠ¥å‘Šè¿›åº¦ï¼šå¼€å§‹è§£æ
        if self.progress_callback:
            self.progress_callback(
                stage='parsing',
                progress=5,
                message=f'å¼€å§‹è§£æ PDFï¼ˆå…± {total_pages} é¡µï¼‰',
                total_pages=total_pages,
                current_page=0
            )

        # ğŸ” æ­¥éª¤1: å…ˆç”¨ PyMuPDF + pdfplumber å°è¯•è§£æ
        logger.info("ğŸš€ ä½¿ç”¨ PyMuPDF + pdfplumber è§£æ...")
        result = self._hybrid_parse(pdf_path, total_pages)

        # ğŸ” æ­¥éª¤2: æ£€æµ‹æ˜¯å¦éœ€è¦ OCR
        needs_ocr = self._check_needs_ocr(result, pdf_path)

        if needs_ocr and MINERU_CLI_AVAILABLE:
            logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ‰«æä»¶æˆ–æ–‡æœ¬æå–ä¸è¶³ï¼Œä½¿ç”¨ MinerU OCR é‡æ–°è§£æ")

            # æŠ¥å‘Šè¿›åº¦ï¼šåˆ‡æ¢åˆ° OCR
            if self.progress_callback:
                self.progress_callback(
                    stage='parsing',
                    progress=10,
                    message=f'æ£€æµ‹åˆ°æ‰«æä»¶ï¼Œä½¿ç”¨ OCR è§£æ...',
                    total_pages=total_pages,
                    current_page=0
                )

            # ä½¿ç”¨ MinerU OCR
            import torch
            device = "cuda" if torch.cuda.is_available() else "cpu"
            logger.info(f"ğŸ”¥ MinerU OCR ä½¿ç”¨è®¾å¤‡: {device}")
            result = self._parse_with_ocr(pdf_path, device, total_pages)
        else:
            logger.info(f"âœ… ä½¿ç”¨æ··åˆæ–¹æ¡ˆæˆåŠŸè§£æï¼ˆæ— éœ€ OCRï¼‰")

        # æŠ¥å‘Šè¿›åº¦ï¼šè§£æå®Œæˆ
        if self.progress_callback:
            self.progress_callback(
                stage='parsing',
                progress=30,
                message=f'PDF è§£æå®Œæˆï¼ˆå…± {total_pages} é¡µï¼‰',
                total_pages=total_pages,
                current_page=total_pages
            )

        return result

    def _hybrid_parse(self, pdf_path: str, total_pages: int) -> Dict[str, Any]:
        """æ··åˆè§£ææ–¹æ³•ï¼šPyMuPDF æå–æ–‡æœ¬ + pdfplumber æå–è¡¨æ ¼

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            total_pages: æ€»é¡µæ•°

        Returns:
            è§£æç»“æœå­—å…¸
        """
        logger.info(f"ä½¿ç”¨æ··åˆæ–¹æ¡ˆè§£æPDF: {pdf_path}")

        # è·å–åŸºæœ¬æ–‡æ¡£ä¿¡æ¯
        doc_info = self._extract_document_info(pdf_path)

        pages_content = []
        tables = []

        try:
            # 1ï¸âƒ£ PyMuPDF æå–æ–‡æœ¬ï¼ˆå¿«é€Ÿï¼‰
            logger.info(f"ğŸ“ ä½¿ç”¨ PyMuPDF æå–æ–‡æœ¬ï¼ˆå…± {total_pages} é¡µï¼‰...")
            start_text_time = time.time()

            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    text = page.get_text()

                    # æŠ¥å‘Šè¿›åº¦
                    if self.progress_callback and page_num % 10 == 0:
                        progress = 5 + int((page_num / total_pages) * 15)  # 5% -> 20%
                        self.progress_callback(
                            stage='parsing',
                            progress=progress,
                            message=f'æå–æ–‡æœ¬ä¸­...',
                            total_pages=total_pages,
                            current_page=page_num
                        )

                    page_content = {
                        "page_number": page_num + 1,
                        "text": text,
                        "tables": [],
                        "images": [],
                        "layout": {},
                        "metadata": {},
                    }
                    pages_content.append(page_content)

            text_elapsed = time.time() - start_text_time
            logger.info(f"âœ… æ–‡æœ¬æå–å®Œæˆï¼Œè€—æ—¶ {text_elapsed:.2f} ç§’")

            # 2ï¸âƒ£ pdfplumber æå–è¡¨æ ¼ï¼ˆç²¾ç¡®ï¼‰
            logger.info(f"ğŸ“Š ä½¿ç”¨ pdfplumber æå–è¡¨æ ¼ï¼ˆå…± {total_pages} é¡µï¼‰...")
            start_table_time = time.time()

            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    # æŠ¥å‘Šè¿›åº¦
                    if self.progress_callback and page_num % 10 == 0:
                        progress = 20 + int((page_num / total_pages) * 8)  # 20% -> 28%
                        self.progress_callback(
                            stage='parsing',
                            progress=progress,
                            message=f'æå–è¡¨æ ¼ä¸­...',
                            total_pages=total_pages,
                            current_page=page_num
                        )

                    page_tables = page.extract_tables()
                    if page_tables:
                        for table_idx, table in enumerate(page_tables):
                            table_data = {
                                "table_id": f"table_{page_num}_{table_idx}",
                                "page_number": page_num + 1,
                                "data": table,
                                "bbox": None,
                                "metadata": {},
                            }
                            tables.append(table_data)
                            pages_content[page_num]["tables"].append(table_data)

            table_elapsed = time.time() - start_table_time
            logger.info(f"âœ… è¡¨æ ¼æå–å®Œæˆï¼Œè€—æ—¶ {table_elapsed:.2f} ç§’ï¼Œå…±æå– {len(tables)} ä¸ªè¡¨æ ¼")

        except Exception as e:
            logger.error(f"æ··åˆè§£æå¤±è´¥: {pdf_path}, é”™è¯¯: {str(e)}")
            # è¿”å›æœ€å°å¯ç”¨ç»“æœ
            pages_content = [
                {
                    "page_number": 1,
                    "text": "",
                    "tables": [],
                    "images": [],
                    "layout": {},
                    "metadata": {"error": str(e)},
                }
            ]

        # æ„å»ºç»“æœ
        all_text = "\n".join([page["text"] for page in pages_content])

        parsed_result = {
            "document_info": doc_info,
            "pages": pages_content,
            "tables": tables,
            "images": [],
            "text_content": all_text,
            "metadata": {
                "parser": "Hybrid (PyMuPDF + pdfplumber)",
                "parser_version": "latest",
                "parse_time": datetime.now().isoformat(),
                "total_pages": len(pages_content),
                "has_tables": len(tables) > 0,
                "has_images": False,
            },
        }

        # è¾“å‡ºè§£æç»Ÿè®¡
        logger.info(f"ğŸ“Š æ··åˆè§£æç»Ÿè®¡:")
        logger.info(f"   - æ€»é¡µæ•°: {len(pages_content)}")
        logger.info(f"   - æ€»å­—ç¬¦æ•°: {len(all_text)}")
        logger.info(f"   - è¡¨æ ¼æ•°: {len(tables)}")
        logger.info(f"   - å¹³å‡æ¯é¡µå­—ç¬¦æ•°: {len(all_text) / len(pages_content):.0f}")

        return parsed_result

    def _check_needs_ocr(self, parsed_result: Dict[str, Any], pdf_path: str) -> bool:
        """æ£€æµ‹PDFæ˜¯å¦éœ€è¦OCR

        åˆ¤æ–­æ ‡å‡†ï¼š
        1. æ–‡æœ¬æå–é‡è¿‡å°‘ï¼ˆå¯èƒ½æ˜¯æ‰«æä»¶ï¼‰
        2. æ–‡æœ¬å¯†åº¦è¿‡ä½

        Args:
            parsed_result: è§£æç»“æœ
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            æ˜¯å¦éœ€è¦OCR
        """
        text_content = parsed_result.get("text_content", "")
        total_pages = parsed_result["metadata"]["total_pages"]

        # è®¡ç®—æ–‡æœ¬é‡
        text_length = len(text_content.strip())
        avg_text_per_page = text_length / total_pages if total_pages > 0 else 0

        logger.info(f"ğŸ“Š æ–‡æœ¬ç»Ÿè®¡: æ€»å­—ç¬¦æ•°={text_length}, å¹³å‡æ¯é¡µ={avg_text_per_page:.0f}å­—ç¬¦")

        # åˆ¤æ–­æ ‡å‡†ï¼šå¹³å‡æ¯é¡µå°‘äº100ä¸ªå­—ç¬¦ï¼Œå¯èƒ½æ˜¯æ‰«æä»¶
        if avg_text_per_page < 100:
            logger.warning(f"âš ï¸ æ–‡æœ¬å¯†åº¦è¿‡ä½ï¼ˆ{avg_text_per_page:.0f}å­—ç¬¦/é¡µï¼‰ï¼Œåˆ¤å®šä¸ºæ‰«æä»¶")
            return True

        # æ£€æŸ¥æ˜¯å¦æœ‰å¤§é‡ä¹±ç æˆ–ç‰¹æ®Šå­—ç¬¦
        import re
        # ç»Ÿè®¡ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—çš„æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text_content))
        english_chars = len(re.findall(r'[a-zA-Z]', text_content))
        digits = len(re.findall(r'\d', text_content))
        valid_chars = chinese_chars + english_chars + digits

        valid_ratio = valid_chars / text_length if text_length > 0 else 0

        logger.info(f"ğŸ“Š æœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹: {valid_ratio:.2%} (ä¸­æ–‡={chinese_chars}, è‹±æ–‡={english_chars}, æ•°å­—={digits})")

        # å¦‚æœæœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹ä½äº30%ï¼Œå¯èƒ½éœ€è¦OCR
        if valid_ratio < 0.3 and text_length > 100:
            logger.warning(f"âš ï¸ æœ‰æ•ˆå­—ç¬¦æ¯”ä¾‹è¿‡ä½ï¼ˆ{valid_ratio:.2%}ï¼‰ï¼Œå¯èƒ½éœ€è¦OCR")
            return True

        logger.info(f"âœ… æ–‡æœ¬æå–æ­£å¸¸ï¼Œæ— éœ€OCR")
        return False

    def _parse_with_ocr(self, pdf_path: str, device: str, total_pages: int) -> Dict[str, Any]:
        """ä½¿ç”¨ MinerU OCR è§£æï¼ˆæ‰«æä»¶ï¼‰"""
        temp_dir = tempfile.mkdtemp(prefix="mineru_ocr_")

        try:
            start_time = time.time()

            cmd = [
                "mineru",
                "-p", pdf_path,
                "-o", temp_dir,
                "-m", "ocr",    # ä½¿ç”¨ OCR æ¨¡å¼
                "-b", "pipeline",
                "-d", device,
                "-f", "false",  # å…³é—­å…¬å¼è¯†åˆ«
                "-t", "true",   # ä¿ç•™è¡¨æ ¼è¯†åˆ«
            ]

            logger.info(f"æ‰§è¡Œ OCR å‘½ä»¤: {' '.join(cmd)}")

            # ä½¿ç”¨ Popen ä»¥ä¾¿å®æ—¶æ›´æ–°è¿›åº¦
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )

            # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°
            import threading
            progress_stop = threading.Event()

            def update_progress_periodically():
                """å®šæœŸæ›´æ–°è¿›åº¦ï¼ˆOCR è¾ƒæ…¢ï¼‰"""
                current_progress = 12
                while not progress_stop.is_set() and current_progress < 28:
                    time.sleep(3)  # OCR è¾ƒæ…¢ï¼Œæ¯3ç§’æ›´æ–°ä¸€æ¬¡
                    if not progress_stop.is_set():
                        current_progress += 2
                        if self.progress_callback and total_pages > 0:
                            estimated_page = int((current_progress - 10) / 20 * total_pages)
                            self.progress_callback(
                                stage='parsing',
                                progress=current_progress,
                                message=f'OCR è¯†åˆ«ä¸­...',
                                total_pages=total_pages,
                                current_page=estimated_page
                            )

            # å¯åŠ¨è¿›åº¦æ›´æ–°çº¿ç¨‹
            if self.progress_callback and total_pages > 0:
                progress_thread = threading.Thread(target=update_progress_periodically, daemon=True)
                progress_thread.start()

            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            try:
                stdout, stderr = process.communicate(timeout=1200)  # OCR è¶…æ—¶æ—¶é—´æ›´é•¿ï¼š20åˆ†é’Ÿ
                returncode = process.returncode
            except subprocess.TimeoutExpired:
                process.kill()
                progress_stop.set()
                logger.error(f"âŒ MinerU OCR è¶…æ—¶ï¼ˆ>1200ç§’ï¼‰")
                # OCR å¤±è´¥ï¼Œè¿”å›ä¹‹å‰çš„æ··åˆè§£æç»“æœ
                return self._hybrid_parse(pdf_path, total_pages)
            finally:
                progress_stop.set()

            if returncode != 0:
                logger.error(f"âŒ MinerU OCR å¤±è´¥: {stderr[:500]}")
                # OCR å¤±è´¥ï¼Œè¿”å›ä¹‹å‰çš„æ··åˆè§£æç»“æœ
                return self._hybrid_parse(pdf_path, total_pages)

            parsed_content = self._read_mineru_output(temp_dir, pdf_path)

            # æ›´æ–° metadata æ ‡è®°ä¸º OCR
            parsed_content["metadata"]["parser"] = "MinerU OCR"
            parsed_content["metadata"]["ocr_used"] = True

            elapsed = time.time() - start_time
            logger.info(f"âœ… OCR è§£æå®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")

            return parsed_content

        finally:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)

    def _parse_serial(self, pdf_path: str, device: str, total_pages: int = 0) -> Dict[str, Any]:
        """ä¸²è¡Œè§£æï¼ˆå°æ–‡ä»¶æˆ– GPU æ¨¡å¼ï¼‰"""
        temp_dir = tempfile.mkdtemp(prefix="mineru_serial_")

        try:
            start_time = time.time()

            cmd = [
                "mineru",
                "-p", pdf_path,
                "-o", temp_dir,
                "-m", "txt",    # ä½¿ç”¨æ–‡æœ¬æ¨¡å¼ï¼ˆä¸ä½¿ç”¨ OCRï¼‰â†’ é€Ÿåº¦æå‡ 10 å€
                "-b", "pipeline",
                "-d", device,
                "-f", "false",  # å…³é—­å…¬å¼è¯†åˆ«
                "-t", "true",   # ä¿ç•™è¡¨æ ¼è¯†åˆ«
            ]

            logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")

            # ä½¿ç”¨ Popen ä»¥ä¾¿å®æ—¶æ›´æ–°è¿›åº¦
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                encoding='utf-8',
                errors='ignore'
            )

            # æ¨¡æ‹Ÿè¿›åº¦æ›´æ–°ï¼ˆå› ä¸º MinerU æ²¡æœ‰å®æ—¶è¿›åº¦è¾“å‡ºï¼‰
            import threading
            progress_stop = threading.Event()

            def update_progress_periodically():
                """å®šæœŸæ›´æ–°è¿›åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰"""
                current_progress = 10
                while not progress_stop.is_set() and current_progress < 28:
                    time.sleep(2)  # æ¯2ç§’æ›´æ–°ä¸€æ¬¡
                    if not progress_stop.is_set():
                        current_progress += 2
                        if self.progress_callback and total_pages > 0:
                            # ä¼°ç®—å½“å‰é¡µæ•°
                            estimated_page = int((current_progress - 5) / 25 * total_pages)
                            self.progress_callback(
                                stage='parsing',
                                progress=current_progress,
                                message=f'æ­£åœ¨è§£æ PDF...',
                                total_pages=total_pages,
                                current_page=estimated_page
                            )

            # å¯åŠ¨è¿›åº¦æ›´æ–°çº¿ç¨‹
            if self.progress_callback and total_pages > 0:
                progress_thread = threading.Thread(target=update_progress_periodically, daemon=True)
                progress_thread.start()

            # ç­‰å¾…è¿›ç¨‹å®Œæˆ
            try:
                stdout, stderr = process.communicate(timeout=600)
                returncode = process.returncode
            except subprocess.TimeoutExpired:
                process.kill()
                progress_stop.set()
                logger.error(f"âŒ MinerU è§£æè¶…æ—¶ï¼ˆ>600ç§’ï¼‰")
                return self._fallback_parse(pdf_path)
            finally:
                progress_stop.set()

            if returncode != 0:
                logger.error(f"âŒ MinerU è§£æå¤±è´¥: {stderr[:500]}")
                return self._fallback_parse(pdf_path)

            parsed_content = self._read_mineru_output(temp_dir, pdf_path)

            elapsed = time.time() - start_time
            logger.info(f"âœ… ä¸²è¡Œè§£æå®Œæˆï¼Œè€—æ—¶ {elapsed:.2f} ç§’")

            return parsed_content

        finally:
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)

    def _read_mineru_output(self, output_dir: str, pdf_path: str) -> Dict[str, Any]:
        """è¯»å– MinerU 2.5.4+ è¾“å‡ºçš„æ–‡ä»¶å¹¶è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼

        Args:
            output_dir: MinerU è¾“å‡ºç›®å½•
            pdf_path: åŸå§‹ PDF æ–‡ä»¶è·¯å¾„

        Returns:
            æ ‡å‡†åŒ–çš„è§£æç»“æœ
        """
        import os
        import glob

        # è·å–åŸºæœ¬æ–‡æ¡£ä¿¡æ¯
        doc_info = self._extract_document_info(pdf_path)

        # mineru 2.5.4 è¾“å‡ºç»“æ„ï¼š
        # output_dir/
        #   {pdf_basename}/        â† PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        #     auto/                â† æ¨¡å¼ç›®å½•ï¼ˆauto/txt/ocrï¼‰
        #       {pdf_basename}.md  â† Markdownæ–‡ä»¶
        #       images/            â† å›¾ç‰‡ç›®å½•

        pdf_basename = Path(pdf_path).stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å

        # æŒ‰ä¼˜å…ˆçº§æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
        search_patterns = [
            # æ¨¡å¼1: {output_dir}/{pdf_basename}/auto/{pdf_basename}.md
            os.path.join(output_dir, pdf_basename, "auto", f"{pdf_basename}.md"),
            # æ¨¡å¼2: {output_dir}/{pdf_basename}/txt/{pdf_basename}.md
            os.path.join(output_dir, pdf_basename, "txt", f"{pdf_basename}.md"),
            # æ¨¡å¼3: {output_dir}/{pdf_basename}/ocr/{pdf_basename}.md
            os.path.join(output_dir, pdf_basename, "ocr", f"{pdf_basename}.md"),
            # æ¨¡å¼4: é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .md æ–‡ä»¶
            os.path.join(output_dir, "**", "*.md"),
        ]

        md_files = []
        for pattern in search_patterns:
            if "**" in pattern:
                # ä½¿ç”¨ glob é€’å½’æŸ¥æ‰¾
                md_files = glob.glob(pattern, recursive=True)
            else:
                # ç›´æ¥æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(pattern):
                    md_files = [pattern]

            if md_files:
                logger.info(f"æ‰¾åˆ° Markdown æ–‡ä»¶: {md_files[0]}")
                break

        if not md_files:
            # åˆ—å‡ºå®é™…ç”Ÿæˆçš„æ–‡ä»¶ï¼Œå¸®åŠ©è°ƒè¯•
            all_files = []
            for root, dirs, files in os.walk(output_dir):
                for f in files:
                    all_files.append(os.path.join(root, f))

            logger.error(f"MinerUè¾“å‡ºç›®å½•ç»“æ„:")
            for f in all_files[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªæ–‡ä»¶
                logger.error(f"  - {os.path.relpath(f, output_dir)}")

            raise FileNotFoundError(
                f"æœªæ‰¾åˆ° MinerU è¾“å‡ºçš„ Markdown æ–‡ä»¶ã€‚"
                f"è¾“å‡ºç›®å½•: {output_dir}ï¼Œ"
                f"PDF basename: {pdf_basename}ï¼Œ"
                f"å®é™…æ–‡ä»¶æ•°: {len(all_files)}"
            )

        # è¯»å– Markdown å†…å®¹
        md_content = ""
        for md_file in md_files:
            with open(md_file, 'r', encoding='utf-8') as f:
                md_content += f.read() + "\n\n"

        # ç®€åŒ–å¤„ç†ï¼šå°† Markdown æŒ‰æ®µè½åˆ†é¡µ
        pages_content = []
        paragraphs = md_content.split('\n\n')

        for idx, para in enumerate(paragraphs):
            if para.strip():
                pages_content.append({
                    "page_number": idx + 1,
                    "text": para.strip(),
                    "tables": [],
                    "images": [],
                    "layout": [],
                    "metadata": {},
                })

        # æ„å»ºæ ‡å‡†åŒ–è¾“å‡º
        parsed_result = {
            "document_info": doc_info,
            "pages": pages_content,
            "tables": [],
            "images": [],
            "text_content": md_content,
            "metadata": {
                "parser": "MinerU-CLI",
                "parser_version": "1.3.12",
                "parse_time": datetime.now().isoformat(),
                "total_pages": len(pages_content),
                "has_tables": False,
                "has_images": False,
                "language": "zh",
            },
        }

        return parsed_result

    def _fallback_parse(self, pdf_path: str) -> Dict[str, Any]:
        """å¤‡ç”¨PDFè§£ææ–¹æ³•ï¼ˆä½¿ç”¨PyMuPDF + pdfplumberï¼‰

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            è§£æç»“æœå­—å…¸
        """
        logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•è§£æPDF: {pdf_path}")

        # è·å–åŸºæœ¬æ–‡æ¡£ä¿¡æ¯
        doc_info = self._extract_document_info(pdf_path)

        # ä½¿ç”¨PyMuPDFæå–æ–‡æœ¬
        pages_content = []
        tables = []

        try:
            # PyMuPDFè§£ææ–‡æœ¬
            with fitz.open(pdf_path) as doc:
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    text = page.get_text()

                    page_content = {
                        "page_number": page_num + 1,
                        "text": text,
                        "tables": [],
                        "images": [],
                        "layout": {},
                        "metadata": {},
                    }
                    pages_content.append(page_content)

            # ä½¿ç”¨pdfplumberæå–è¡¨æ ¼
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_tables = page.extract_tables()
                    if page_tables:
                        for table_idx, table in enumerate(page_tables):
                            table_data = {
                                "table_id": f"table_{page_num}_{table_idx}",
                                "page_number": page_num + 1,
                                "data": table,
                                "bbox": None,  # pdfplumberä¸ç›´æ¥æä¾›bbox
                                "metadata": {},
                            }
                            tables.append(table_data)
                            pages_content[page_num]["tables"].append(table_data)

        except Exception as e:
            logger.error(f"å¤‡ç”¨è§£æä¹Ÿå¤±è´¥: {pdf_path}, é”™è¯¯: {str(e)}")
            # è¿”å›æœ€å°å¯ç”¨ç»“æœ
            pages_content = [
                {
                    "page_number": 1,
                    "text": "",
                    "tables": [],
                    "images": [],
                    "layout": {},
                    "metadata": {"error": str(e)},
                }
            ]

        # æ„å»ºç»“æœ
        all_text = "\n".join([page["text"] for page in pages_content])

        parsed_result = {
            "document_info": doc_info,
            "pages": pages_content,
            "tables": tables,
            "images": [],
            "text_content": all_text,
            "metadata": {
                "parser": "Fallback (PyMuPDF + pdfplumber)",
                "parser_version": "latest",
                "parse_time": datetime.now().isoformat(),
                "total_pages": len(pages_content),
                "has_tables": len(tables) > 0,
                "has_images": False,
                "language": "zh",
            },
        }

        return parsed_result

    def _extract_document_info(self, pdf_path: str) -> Dict[str, Any]:
        """æå–PDFæ–‡æ¡£åŸºæœ¬ä¿¡æ¯

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡æ¡£ä¿¡æ¯å­—å…¸
        """
        file_path = Path(pdf_path)
        doc_info = {
            "file_name": file_path.name,
            "file_path": str(file_path),
            "file_size": file_path.stat().st_size,
            "creation_time": datetime.fromtimestamp(
                file_path.stat().st_ctime
            ).isoformat(),
            "modification_time": datetime.fromtimestamp(
                file_path.stat().st_mtime
            ).isoformat(),
        }

        # å°è¯•è·å–PDFå…ƒæ•°æ®
        try:
            with fitz.open(pdf_path) as doc:
                metadata = doc.metadata
                doc_info.update(
                    {
                        "title": metadata.get("title", ""),
                        "author": metadata.get("author", ""),
                        "subject": metadata.get("subject", ""),
                        "creator": metadata.get("creator", ""),
                        "producer": metadata.get("producer", ""),
                        "creation_date": metadata.get("creationDate", ""),
                        "modification_date": metadata.get("modDate", ""),
                        "total_pages": doc.page_count,
                    }
                )
        except Exception as e:
            logger.warning(f"æ— æ³•è·å–PDFå…ƒæ•°æ®: {pdf_path}, é”™è¯¯: {str(e)}")
            doc_info["total_pages"] = 0

        return doc_info


class PDFParsingPipeline:
    """PDFè§£ææµæ°´çº¿"""

    def __init__(self, scenario_id: str = "investment", progress_callback=None):
        self.parser = MinerUPDFParser(scenario_id=scenario_id, progress_callback=progress_callback)
        self.settings = get_settings()
        self.scenario_id = scenario_id

    def parse_pdf_file(
        self, pdf_path: str, output_dir: Optional[str] = None
    ) -> Tuple[bool, Dict[str, Any]]:
        """è§£æå•ä¸ªPDFæ–‡ä»¶

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºdebug_data/parsed_reports

        Returns:
            (æ˜¯å¦æˆåŠŸ, è§£æç»“æœæˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            # è®¾ç½®è¾“å‡ºç›®å½•
            if output_dir is None:
                output_dir = self.settings.debug_dir / "parsed_reports"
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶ID
            file_id = self._generate_file_id(pdf_path)

            # æ‰§è¡Œè§£æ
            logger.info(f"å¼€å§‹è§£æPDFæ–‡ä»¶: {pdf_path}")
            parsed_result = self.parser.parse_pdf_with_mineru(pdf_path)

            # æ·»åŠ å¤„ç†ä¿¡æ¯
            parsed_result["processing_info"] = {
                "file_id": file_id,
                "input_path": pdf_path,
                "processed_at": datetime.now().isoformat(),
                "processor": "MinerUPDFParser",
                "scenario_id": self.scenario_id,
            }

            # æ ¹æ®åœºæ™¯è¿›è¡Œç‰¹å®šå¤„ç†
            if self.scenario_id == "tender":
                parsed_result = self._extract_tender_metadata(parsed_result)
            elif self.scenario_id == "investment":
                parsed_result = self._extract_investment_metadata(parsed_result)

            # ä¿å­˜è§£æç»“æœ
            output_file = output_dir / f"{file_id}_parsed.json"
            with open(output_file, "w", encoding="utf-8") as f:
                json.dump(parsed_result, f, ensure_ascii=False, indent=2)

            logger.info(f"PDFè§£æå®Œæˆ: {pdf_path} -> {output_file}")

            return True, {
                "file_id": file_id,
                "output_path": str(output_file),
                "parsed_result": parsed_result,
            }

        except Exception as e:
            error_msg = f"PDFè§£æå¤±è´¥: {pdf_path}, é”™è¯¯: {str(e)}"
            logger.error(error_msg)
            return False, {"error": error_msg}

    def batch_parse_pdfs(
        self, pdf_directory: str, output_dir: Optional[str] = None
    ) -> Dict[str, Any]:
        """æ‰¹é‡è§£æPDFæ–‡ä»¶

        Args:
            pdf_directory: PDFæ–‡ä»¶ç›®å½•
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        pdf_dir = Path(pdf_directory)
        pdf_files = list(pdf_dir.glob("*.pdf"))

        logger.info(f"å‘ç° {len(pdf_files)} ä¸ªPDFæ–‡ä»¶å¾…å¤„ç†")

        results = {
            "total_files": len(pdf_files),
            "successful": 0,
            "failed": 0,
            "results": [],
        }

        for pdf_file in pdf_files:
            success, result = self.parse_pdf_file(str(pdf_file), output_dir)

            if success:
                results["successful"] += 1
            else:
                results["failed"] += 1

            results["results"].append(
                {"file_path": str(pdf_file), "success": success, "result": result}
            )

        logger.info(
            f"æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {results['successful']}, å¤±è´¥ {results['failed']}"
        )
        return results

    def _generate_file_id(self, pdf_path: str) -> str:
        """ç”Ÿæˆæ–‡ä»¶ID

        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„

        Returns:
            æ–‡ä»¶ID
        """
        file_path = Path(pdf_path)
        # ä½¿ç”¨æ–‡ä»¶åå’Œæ—¶é—´æˆ³ç”Ÿæˆå”¯ä¸€ID
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = file_path.stem
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ä¸‹åˆ’çº¿
        clean_name = "".join(
            c for c in base_name if c.isalnum() or c in ["_", "-", "ä¸­"]
        )
        return f"{clean_name}_{timestamp}"

    def _extract_tender_metadata(self, parsed_result: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ‹›æŠ•æ ‡æ–‡æ¡£çš„ç‰¹å®šå…ƒæ•°æ®"""
        try:
            text_content = parsed_result.get("text_content", "")
            metadata = parsed_result.get("metadata", {})

            # æ‹›æŠ•æ ‡ç‰¹å®šä¿¡æ¯æå–
            tender_metadata = {
                "document_type": "tender",
                "extracted_info": {}
            }

            # æå–æ‹›æ ‡ç¼–å·
            import re
            tender_number_patterns = [
                r'æ‹›æ ‡ç¼–å·[ï¼š:]\s*([A-Z0-9\-_]+)',
                r'é¡¹ç›®ç¼–å·[ï¼š:]\s*([A-Z0-9\-_]+)',
                r'æ ‡ä¹¦ç¼–å·[ï¼š:]\s*([A-Z0-9\-_]+)'
            ]

            for pattern in tender_number_patterns:
                match = re.search(pattern, text_content)
                if match:
                    tender_metadata["extracted_info"]["tender_number"] = match.group(1)
                    break

            # æå–æˆªæ­¢æ—¶é—´
            time_patterns = [
                r'æŠ•æ ‡æˆªæ­¢æ—¶é—´[ï¼š:]\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*?\d{1,2}[:ï¼š]\d{2})',
                r'å¼€æ ‡æ—¶é—´[ï¼š:]\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*?\d{1,2}[:ï¼š]\d{2})',
                r'é€’äº¤æŠ•æ ‡æ–‡ä»¶æˆªæ­¢æ—¶é—´[ï¼š:]\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*?\d{1,2}[:ï¼š]\d{2})'
            ]

            for pattern in time_patterns:
                match = re.search(pattern, text_content)
                if match:
                    tender_metadata["extracted_info"]["deadline"] = match.group(1)
                    break

            # æå–é¢„ç®—ä¿¡æ¯
            budget_patterns = [
                r'é¢„ç®—é‡‘é¢[ï¼š:]\s*([\d,]+\.?\d*)\s*ä¸‡?å…ƒ',
                r'æŠ•èµ„æ€»é¢[ï¼š:]\s*([\d,]+\.?\d*)\s*ä¸‡?å…ƒ',
                r'é¡¹ç›®æ€»æŠ•èµ„[ï¼š:]\s*([\d,]+\.?\d*)\s*ä¸‡?å…ƒ'
            ]

            for pattern in budget_patterns:
                match = re.search(pattern, text_content)
                if match:
                    tender_metadata["extracted_info"]["budget"] = match.group(1)
                    break

            # æå–èµ„è´¨è¦æ±‚
            if "èµ„è´¨è¦æ±‚" in text_content or "æŠ•æ ‡äººèµ„æ ¼" in text_content:
                tender_metadata["extracted_info"]["has_qualification_requirements"] = True

            # æå–æŠ€æœ¯è¦æ±‚
            if "æŠ€æœ¯è¦æ±‚" in text_content or "æŠ€æœ¯è§„èŒƒ" in text_content:
                tender_metadata["extracted_info"]["has_technical_requirements"] = True

            # åˆå¹¶åˆ°åŸæœ‰å…ƒæ•°æ®
            metadata.update(tender_metadata)
            parsed_result["metadata"] = metadata

            logger.info(f"æ‹›æŠ•æ ‡å…ƒæ•°æ®æå–å®Œæˆ: {tender_metadata['extracted_info']}")

        except Exception as e:
            logger.error(f"æ‹›æŠ•æ ‡å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}")

        return parsed_result

    def _extract_investment_metadata(self, parsed_result: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æŠ•ç ”æ–‡æ¡£çš„ç‰¹å®šå…ƒæ•°æ®"""
        try:
            text_content = parsed_result.get("text_content", "")
            metadata = parsed_result.get("metadata", {})

            # æŠ•ç ”ç‰¹å®šä¿¡æ¯æå–
            investment_metadata = {
                "document_type": "investment",
                "extracted_info": {}
            }

            # æå–å…¬å¸ä¿¡æ¯
            import re
            company_patterns = [
                r'([^ï¼Œã€‚\s]+)(?:è‚¡ä»½æœ‰é™å…¬å¸|æœ‰é™å…¬å¸|é›†å›¢|ç§‘æŠ€|ç”µå­|æ–°èƒ½æº)',
                r'å…¬å¸åç§°[ï¼š:]\s*([^ï¼Œã€‚\n]+)',
                r'æ ‡çš„å…¬å¸[ï¼š:]\s*([^ï¼Œã€‚\n]+)'
            ]

            for pattern in company_patterns:
                matches = re.findall(pattern, text_content)
                if matches:
                    investment_metadata["extracted_info"]["companies"] = list(set(matches))
                    break

            # æå–æŠ•èµ„è¯„çº§
            rating_patterns = [
                r'æŠ•èµ„è¯„çº§[ï¼š:]\s*(ä¹°å…¥|å¢æŒ|æŒæœ‰|å‡æŒ|å–å‡º)',
                r'è¯„çº§[ï¼š:]\s*(ä¹°å…¥|å¢æŒ|æŒæœ‰|å‡æŒ|å–å‡º)',
                r'å»ºè®®[ï¼š:]\s*(ä¹°å…¥|å¢æŒ|æŒæœ‰|å‡æŒ|å–å‡º)'
            ]

            for pattern in rating_patterns:
                match = re.search(pattern, text_content)
                if match:
                    investment_metadata["extracted_info"]["rating"] = match.group(1)
                    break

            # æå–ç›®æ ‡ä»·æ ¼
            price_patterns = [
                r'ç›®æ ‡ä»·[ï¼š:]\s*([\d.]+)\s*å…ƒ',
                r'ç›®æ ‡ä»·æ ¼[ï¼š:]\s*([\d.]+)\s*å…ƒ',
                r'åˆç†ä»·å€¼[ï¼š:]\s*([\d.]+)\s*å…ƒ'
            ]

            for pattern in price_patterns:
                match = re.search(pattern, text_content)
                if match:
                    investment_metadata["extracted_info"]["target_price"] = float(match.group(1))
                    break

            # æå–æŠ¥å‘Šç±»å‹
            if "æ·±åº¦ç ”ç©¶" in text_content:
                investment_metadata["extracted_info"]["report_type"] = "æ·±åº¦ç ”ç©¶"
            elif "å­£æŠ¥" in text_content:
                investment_metadata["extracted_info"]["report_type"] = "å­£æŠ¥ç‚¹è¯„"
            elif "å¹´æŠ¥" in text_content:
                investment_metadata["extracted_info"]["report_type"] = "å¹´æŠ¥ç‚¹è¯„"

            # åˆå¹¶åˆ°åŸæœ‰å…ƒæ•°æ®
            metadata.update(investment_metadata)
            parsed_result["metadata"] = metadata

            logger.info(f"æŠ•ç ”å…ƒæ•°æ®æå–å®Œæˆ: {investment_metadata['extracted_info']}")

        except Exception as e:
            logger.error(f"æŠ•ç ”å…ƒæ•°æ®æå–å¤±è´¥: {str(e)}")

        return parsed_result


# ä¾¿æ·å‡½æ•°
def parse_single_pdf(pdf_path: str, output_dir: Optional[str] = None, scenario_id: str = "investment") -> Dict[str, Any]:
    """è§£æå•ä¸ªPDFæ–‡ä»¶çš„ä¾¿æ·å‡½æ•°

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        scenario_id: åœºæ™¯IDï¼Œé»˜è®¤ä¸ºæŠ•ç ”åœºæ™¯

    Returns:
        è§£æç»“æœ
    """
    pipeline = PDFParsingPipeline(scenario_id=scenario_id)
    success, result = pipeline.parse_pdf_file(pdf_path, output_dir)

    if success:
        return result
    else:
        raise Exception(result.get("error", "PDFè§£æå¤±è´¥"))


def batch_parse_pdfs(
    pdf_directory: str, output_dir: Optional[str] = None, scenario_id: str = "investment"
) -> Dict[str, Any]:
    """æ‰¹é‡è§£æPDFæ–‡ä»¶çš„ä¾¿æ·å‡½æ•°

    Args:
        pdf_directory: PDFæ–‡ä»¶ç›®å½•
        output_dir: è¾“å‡ºç›®å½•
        scenario_id: åœºæ™¯IDï¼Œé»˜è®¤ä¸ºæŠ•ç ”åœºæ™¯

    Returns:
        æ‰¹é‡å¤„ç†ç»“æœ
    """
    pipeline = PDFParsingPipeline(scenario_id=scenario_id)
    return pipeline.batch_parse_pdfs(pdf_directory, output_dir)
