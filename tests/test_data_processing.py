"""
æ•°æ®æ¸…æ´—ä¸å­˜å‚¨æµ‹è¯•
éªŒè¯T2.1.3ä»»åŠ¡ï¼šæ•°æ®æ¸…æ´—ä¸å­˜å‚¨
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pytest
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TestAdvancedDataCleaner:
    """é«˜çº§æ•°æ®æ¸…æ´—å™¨æµ‹è¯•"""

    def test_cleaner_exists(self):
        """æµ‹è¯•1: æ•°æ®æ¸…æ´—å™¨å­˜åœ¨"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()
        assert cleaner is not None

        logger.info("âœ… æ•°æ®æ¸…æ´—å™¨éªŒè¯é€šè¿‡")

    def test_amount_cleaning(self):
        """æµ‹è¯•2: é‡‘é¢æ¸…æ´—"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()

        # æµ‹è¯•ä¸åŒæ ¼å¼
        assert cleaner.clean_amount("100ä¸‡å…ƒ") == 100.0
        assert cleaner.clean_amount("1000å…ƒ") == 0.1
        assert cleaner.clean_amount("10äº¿å…ƒ") == 100000.0
        assert abs(cleaner.clean_amount("1,234,567.89å…ƒ") - 123.4568) < 0.01  # æµ®ç‚¹æ•°ç²¾åº¦
        assert abs(cleaner.clean_amount("100.5ä¸‡") - 100.5) < 0.01

        logger.info("âœ… é‡‘é¢æ¸…æ´—éªŒè¯é€šè¿‡")

    def test_date_cleaning(self):
        """æµ‹è¯•3: æ—¥æœŸæ¸…æ´—"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()

        # æµ‹è¯•ä¸åŒæ ¼å¼
        assert cleaner.clean_date("2024-01-01 12:00:00") == "2024-01-01 12:00:00"
        assert cleaner.clean_date("2024å¹´1æœˆ1æ—¥") == "2024-01-01"
        assert cleaner.clean_date("2024/01/01") == "2024-01-01"
        assert cleaner.clean_date("20240101") == "2024-01-01"

        logger.info("âœ… æ—¥æœŸæ¸…æ´—éªŒè¯é€šè¿‡")

    def test_province_cleaning(self):
        """æµ‹è¯•4: çœä»½æ¸…æ´—"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()

        # æµ‹è¯•ä¸åŒæ ¼å¼
        assert cleaner.clean_province("åŒ—äº¬") == "åŒ—äº¬å¸‚"
        assert cleaner.clean_province("å¹¿ä¸œ") == "å¹¿ä¸œçœ"
        assert cleaner.clean_province("æ–°ç–†") == "æ–°ç–†ç»´å¾å°”è‡ªæ²»åŒº"
        assert cleaner.clean_province("åŒ—äº¬å¸‚") == "åŒ—äº¬å¸‚"

        logger.info("âœ… çœä»½æ¸…æ´—éªŒè¯é€šè¿‡")

    def test_text_cleaning(self):
        """æµ‹è¯•5: æ–‡æœ¬æ¸…æ´—"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()

        # æµ‹è¯•æ–‡æœ¬æ¸…æ´—
        assert cleaner.clean_text("  å¤šä½™   ç©ºç™½  ") == "å¤šä½™ ç©ºç™½"
        assert cleaner.clean_text("æ¢è¡Œ\n\næµ‹è¯•\r\næ–‡æœ¬") == "æ¢è¡Œ\næµ‹è¯•\næ–‡æœ¬"

        logger.info("âœ… æ–‡æœ¬æ¸…æ´—éªŒè¯é€šè¿‡")

    def test_phone_extraction(self):
        """æµ‹è¯•6: ç”µè¯æå–"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()

        # æµ‹è¯•ç”µè¯æå–
        assert cleaner.extract_phone("è”ç³»ç”µè¯ï¼š13812345678") == "13812345678"
        assert cleaner.extract_phone("ç”µè¯ï¼š010-12345678") == "010-12345678"

        logger.info("âœ… ç”µè¯æå–éªŒè¯é€šè¿‡")

    def test_email_extraction(self):
        """æµ‹è¯•7: é‚®ç®±æå–"""
        from backend.crawler.data_processor import AdvancedDataCleaner

        cleaner = AdvancedDataCleaner()

        # æµ‹è¯•é‚®ç®±æå–
        assert cleaner.extract_email("è”ç³»é‚®ç®±ï¼štest@example.com") == "test@example.com"
        assert cleaner.extract_email("Email: contact@company.com.cn") == "contact@company.com.cn"

        logger.info("âœ… é‚®ç®±æå–éªŒè¯é€šè¿‡")


class TestCrossPlatformDeduplicator:
    """è·¨å¹³å°å»é‡å™¨æµ‹è¯•"""

    def test_deduplicator_exists(self):
        """æµ‹è¯•8: å»é‡å™¨å­˜åœ¨"""
        from backend.crawler.data_processor import CrossPlatformDeduplicator

        dedup = CrossPlatformDeduplicator()
        assert dedup is not None

        logger.info("âœ… å»é‡å™¨éªŒè¯é€šè¿‡")

    def test_url_deduplication(self):
        """æµ‹è¯•9: URLå»é‡"""
        from backend.crawler.data_processor import CrossPlatformDeduplicator

        dedup = CrossPlatformDeduplicator()

        url = "http://example.com/project/123"
        title = "æµ‹è¯•é¡¹ç›®"

        # ç¬¬ä¸€æ¬¡ä¸é‡å¤
        is_dup, reason = dedup.is_duplicate(url, title)
        assert is_dup == False

        # æ ‡è®°ä¸ºå·²è§
        dedup.mark_seen(url, title)

        # ç¬¬äºŒæ¬¡é‡å¤
        is_dup, reason = dedup.is_duplicate(url, title)
        assert is_dup == True
        assert 'URL' in reason

        logger.info("âœ… URLå»é‡éªŒè¯é€šè¿‡")

    def test_similarity_deduplication(self):
        """æµ‹è¯•10: ç›¸ä¼¼åº¦å»é‡"""
        from backend.crawler.data_processor import CrossPlatformDeduplicator

        dedup = CrossPlatformDeduplicator(similarity_threshold=0.85)

        # ä¸¤ä¸ªç›¸ä¼¼çš„æ ‡é¢˜
        url1 = "http://example.com/p1"
        url2 = "http://example.com/p2"  # ä¸åŒURL
        title1 = "æŸæŸç”µåŠ›å…¬å¸æ‹›æ ‡é¡¹ç›®"
        title2 = "æŸæŸç”µåŠ›å…¬å¸æ‹›æ ‡é¡¹ç›®å…¬å‘Š"  # éå¸¸ç›¸ä¼¼
        budget = 100.0

        # ç¬¬ä¸€ä¸ªé¡¹ç›®
        dedup.mark_seen(url1, title1, budget)

        # ç¬¬äºŒä¸ªé¡¹ç›®ï¼ˆæ ‡é¢˜ç›¸ä¼¼ï¼Œé‡‘é¢ç›¸åŒï¼‰
        is_dup, reason = dedup.is_duplicate(url2, title2, budget)
        assert is_dup == True  # åº”è¯¥è¢«è¯†åˆ«ä¸ºé‡å¤

        logger.info("âœ… ç›¸ä¼¼åº¦å»é‡éªŒè¯é€šè¿‡")

    def test_similarity_calculation(self):
        """æµ‹è¯•11: ç›¸ä¼¼åº¦è®¡ç®—"""
        from backend.crawler.data_processor import CrossPlatformDeduplicator

        dedup = CrossPlatformDeduplicator()

        # æµ‹è¯•å®Œå…¨ç›¸åŒ
        sim1 = dedup._calculate_similarity("æµ‹è¯•æ–‡æœ¬", "æµ‹è¯•æ–‡æœ¬")
        assert sim1 == 1.0

        # æµ‹è¯•å®Œå…¨ä¸åŒ
        sim2 = dedup._calculate_similarity("æµ‹è¯•æ–‡æœ¬", "å®Œå…¨ä¸åŒ")
        assert sim2 < 0.5

        # æµ‹è¯•éƒ¨åˆ†ç›¸ä¼¼
        sim3 = dedup._calculate_similarity("æŸæŸå…¬å¸æ‹›æ ‡é¡¹ç›®", "æŸæŸå…¬å¸æ‹›æ ‡é¡¹ç›®å…¬å‘Š")
        assert 0.8 < sim3 < 1.0

        logger.info("âœ… ç›¸ä¼¼åº¦è®¡ç®—éªŒè¯é€šè¿‡")


class TestDataNormalizer:
    """æ•°æ®è§„èŒƒåŒ–å™¨æµ‹è¯•"""

    def test_normalizer_exists(self):
        """æµ‹è¯•12: è§„èŒƒåŒ–å™¨å­˜åœ¨"""
        from backend.crawler.data_processor import DataNormalizer

        normalizer = DataNormalizer()
        assert normalizer is not None

        logger.info("âœ… è§„èŒƒåŒ–å™¨éªŒè¯é€šè¿‡")

    def test_tender_item_normalization(self):
        """æµ‹è¯•13: æ‹›æ ‡é¡¹ç›®è§„èŒƒåŒ–"""
        from backend.crawler.data_processor import DataNormalizer

        normalizer = DataNormalizer()

        # æ¨¡æ‹ŸåŸå§‹æ•°æ®
        raw_data = {
            'title': '  æŸæŸé¡¹ç›®æ‹›æ ‡  ',
            'budget_text': '100ä¸‡å…ƒ',
            'publish_time': '2024å¹´1æœˆ1æ—¥',
            'province': 'åŒ—äº¬',
            'city': 'åŒ—äº¬',
            'content_text': 'é¡¹ç›®å†…å®¹\n\nå¤šä½™ç©ºç™½'
        }

        # è§„èŒƒåŒ–
        normalized = normalizer.normalize_tender_item(raw_data)

        # éªŒè¯ç»“æœ
        assert normalized['title'] == 'æŸæŸé¡¹ç›®æ‹›æ ‡'
        assert normalized['budget'] == 100.0
        assert normalized['publish_time'] == '2024-01-01'
        assert normalized['province'] == 'åŒ—äº¬å¸‚'
        assert normalized['city'] == 'åŒ—äº¬å¸‚'

        logger.info("âœ… æ‹›æ ‡é¡¹ç›®è§„èŒƒåŒ–éªŒè¯é€šè¿‡")

    def test_duplicate_check_integration(self):
        """æµ‹è¯•14: å»é‡æ£€æŸ¥é›†æˆ"""
        from backend.crawler.data_processor import DataNormalizer

        normalizer = DataNormalizer()

        item1 = {
            'source_url': 'http://example.com/p1',
            'title': 'æµ‹è¯•é¡¹ç›®1',
            'budget': 100.0
        }

        item2 = {
            'source_url': 'http://example.com/p2',
            'title': 'æµ‹è¯•é¡¹ç›®1',  # æ ‡é¢˜ç›¸åŒ
            'budget': 100.0  # é‡‘é¢ç›¸åŒ
        }

        # ç¬¬ä¸€ä¸ªä¸é‡å¤
        is_dup1, _ = normalizer.check_duplicate(item1)
        assert is_dup1 == False
        normalizer.mark_processed(item1)

        # ç¬¬äºŒä¸ªåº”è¯¥é‡å¤ï¼ˆæ ‡é¢˜+é‡‘é¢ç›¸ä¼¼ï¼‰
        is_dup2, reason = normalizer.check_duplicate(item2)
        assert is_dup2 == True

        logger.info("âœ… å»é‡æ£€æŸ¥é›†æˆéªŒè¯é€šè¿‡")


class TestEnhancedPipelines:
    """å¢å¼ºç‰ˆPipelineæµ‹è¯•"""

    def test_enhanced_cleaning_pipeline_exists(self):
        """æµ‹è¯•15: å¢å¼ºç‰ˆæ¸…æ´—Pipelineå­˜åœ¨"""
        from backend.crawler.pipelines import EnhancedDataCleaningPipeline

        pipeline = EnhancedDataCleaningPipeline()
        assert pipeline is not None
        assert hasattr(pipeline, 'process_item')

        logger.info("âœ… å¢å¼ºç‰ˆæ¸…æ´—PipelineéªŒè¯é€šè¿‡")

    def test_cross_platform_duplicates_pipeline_exists(self):
        """æµ‹è¯•16: è·¨å¹³å°å»é‡Pipelineå­˜åœ¨"""
        from backend.crawler.pipelines import CrossPlatformDuplicatesPipeline

        pipeline = CrossPlatformDuplicatesPipeline()
        assert pipeline is not None
        assert hasattr(pipeline, 'process_item')

        logger.info("âœ… è·¨å¹³å°å»é‡PipelineéªŒè¯é€šè¿‡")

    def test_pipeline_integration(self):
        """æµ‹è¯•17: Pipelineå®Œæ•´æ€§éªŒè¯"""
        # éªŒè¯æ‰€æœ‰å¢å¼ºç»„ä»¶éƒ½å·²åˆ›å»º
        components = [
            project_root / "backend" / "crawler" / "data_processor.py",
            project_root / "backend" / "crawler" / "pipelines.py",
        ]

        for component in components:
            assert component.exists(), f"ç¼ºå°‘ç»„ä»¶: {component}"

        logger.info("âœ… Pipelineå®Œæ•´æ€§éªŒè¯é€šè¿‡")


if __name__ == "__main__":
    logger.info("ğŸ§ª å¼€å§‹è¿è¡Œæ•°æ®å¤„ç†æµ‹è¯•...")
    pytest.main([__file__, "-v", "-s"])

