"""
çˆ¬è™«å¹³å°å¯¹æ¥æµ‹è¯•
éªŒè¯T2.1.2ä»»åŠ¡ï¼šå¯¹æ¥2-3ä¸ªæ‹›æ ‡å¹³å°
"""

import sys
from pathlib import Path

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import pytest
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TestCrawlerPlatforms:
    """çˆ¬è™«å¹³å°æµ‹è¯•ç±»"""

    def test_gov_procurement_spider_exists(self):
        """æµ‹è¯•1: ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«å­˜åœ¨"""
        spider_file = project_root / "backend" / "crawler" / "spiders" / "gov_procurement_spider.py"
        assert spider_file.exists()

        from backend.crawler.spiders.gov_procurement_spider import GovProcurementSpider
        assert GovProcurementSpider is not None

        spider = GovProcurementSpider()
        assert spider.name == 'gov_procurement'
        assert 'ccgp.gov.cn' in spider.allowed_domains

        logger.info("âœ… ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«éªŒè¯é€šè¿‡")

    def test_gov_procurement_spider_methods(self):
        """æµ‹è¯•2: ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«æ–¹æ³•å®Œæ•´æ€§"""
        from backend.crawler.spiders.gov_procurement_spider import GovProcurementSpider

        spider = GovProcurementSpider()

        # æ£€æŸ¥å…³é”®æ–¹æ³•
        assert hasattr(spider, 'parse_tender_list')
        assert hasattr(spider, 'parse_bid_result_list')
        assert hasattr(spider, 'parse_tender_detail')
        assert hasattr(spider, 'parse_bid_result_detail')
        assert hasattr(spider, '_extract_title')
        assert hasattr(spider, '_extract_budget')
        assert hasattr(spider, '_extract_location')

        logger.info("âœ… ä¸­å›½æ”¿åºœé‡‡è´­ç½‘çˆ¬è™«æ–¹æ³•éªŒè¯é€šè¿‡")

    def test_dynamic_platform_spider_exists(self):
        """æµ‹è¯•3: åŠ¨æ€åŠ è½½å¹³å°çˆ¬è™«å­˜åœ¨"""
        spider_file = project_root / "backend" / "crawler" / "spiders" / "dynamic_platform_spider.py"
        assert spider_file.exists()

        from backend.crawler.spiders.dynamic_platform_spider import DynamicPlatformSpider
        assert DynamicPlatformSpider is not None

        spider = DynamicPlatformSpider()
        assert spider.name == 'dynamic_platform'
        assert spider.use_selenium == True

        logger.info("âœ… åŠ¨æ€åŠ è½½å¹³å°çˆ¬è™«éªŒè¯é€šè¿‡")

    def test_auth_platform_spider_exists(self):
        """æµ‹è¯•4: éœ€ç™»å½•å¹³å°çˆ¬è™«å­˜åœ¨"""
        spider_file = project_root / "backend" / "crawler" / "spiders" / "auth_platform_spider.py"
        assert spider_file.exists()

        from backend.crawler.spiders.auth_platform_spider import AuthPlatformSpider
        assert AuthPlatformSpider is not None

        spider = AuthPlatformSpider()
        assert spider.name == 'auth_platform'
        assert hasattr(spider, 'login_url')
        assert hasattr(spider, 'cookie_file')

        logger.info("âœ… éœ€ç™»å½•å¹³å°çˆ¬è™«éªŒè¯é€šè¿‡")

    def test_auth_spider_cookie_management(self):
        """æµ‹è¯•5: ç™»å½•çˆ¬è™«Cookieç®¡ç†"""
        from backend.crawler.spiders.auth_platform_spider import AuthPlatformSpider

        spider = AuthPlatformSpider()

        # æ£€æŸ¥Cookieç®¡ç†æ–¹æ³•
        assert hasattr(spider, '_load_cookies')
        assert hasattr(spider, '_save_cookies')
        assert hasattr(spider, '_is_cookie_expired')
        assert hasattr(spider, '_check_login_success')
        assert hasattr(spider, '_need_relogin')

        logger.info("âœ… Cookieç®¡ç†æ–¹æ³•éªŒè¯é€šè¿‡")

    def test_incremental_manager_exists(self):
        """æµ‹è¯•6: å¢é‡æ›´æ–°ç®¡ç†å™¨å­˜åœ¨"""
        manager_file = project_root / "backend" / "crawler" / "incremental_manager.py"
        assert manager_file.exists()

        from backend.crawler.incremental_manager import IncrementalManager, get_incremental_manager
        assert IncrementalManager is not None

        manager = get_incremental_manager()
        assert manager is not None
        assert hasattr(manager, 'is_url_crawled')
        assert hasattr(manager, 'mark_url_crawled')

        logger.info("âœ… å¢é‡æ›´æ–°ç®¡ç†å™¨éªŒè¯é€šè¿‡")

    def test_incremental_manager_methods(self):
        """æµ‹è¯•7: å¢é‡æ›´æ–°ç®¡ç†å™¨æ–¹æ³•"""
        from backend.crawler.incremental_manager import get_incremental_manager

        manager = get_incremental_manager()

        # æµ‹è¯•URLæ£€æŸ¥ï¼ˆåº”è¯¥æœªçˆ¬å–ï¼‰
        test_url = 'http://test.example.com/project/12345'
        assert manager.is_url_crawled(test_url) == False

        # æµ‹è¯•æ ‡è®°URL
        manager.mark_url_crawled(test_url, 'test_hash_123', 'test_spider')

        # æµ‹è¯•å“ˆå¸Œæ£€æŸ¥
        assert manager.is_hash_crawled('test_hash_123') == True

        logger.info("âœ… å¢é‡æ›´æ–°ç®¡ç†å™¨åŠŸèƒ½éªŒè¯é€šè¿‡")

    def test_crawler_manager_exists(self):
        """æµ‹è¯•8: çˆ¬è™«ç®¡ç†å™¨å­˜åœ¨"""
        manager_file = project_root / "backend" / "crawler" / "crawler_manager.py"
        assert manager_file.exists()

        from backend.crawler.crawler_manager import CrawlerManager, get_crawler_manager
        assert CrawlerManager is not None

        manager = get_crawler_manager()
        assert manager is not None

        logger.info("âœ… çˆ¬è™«ç®¡ç†å™¨éªŒè¯é€šè¿‡")

    def test_crawler_manager_registration(self):
        """æµ‹è¯•9: çˆ¬è™«ç®¡ç†å™¨æ³¨å†Œ"""
        from backend.crawler.crawler_manager import get_crawler_manager

        manager = get_crawler_manager()

        # æ£€æŸ¥å·²æ³¨å†Œçš„çˆ¬è™«
        assert 'gov_procurement' in manager.registered_spiders
        assert 'dynamic_platform' in manager.registered_spiders
        assert 'auth_platform' in manager.registered_spiders
        assert 'demo_tender' in manager.registered_spiders

        logger.info("âœ… çˆ¬è™«æ³¨å†ŒéªŒè¯é€šè¿‡")

    def test_crawler_manager_methods(self):
        """æµ‹è¯•10: çˆ¬è™«ç®¡ç†å™¨æ–¹æ³•"""
        from backend.crawler.crawler_manager import get_crawler_manager

        manager = get_crawler_manager()

        # æ£€æŸ¥å…³é”®æ–¹æ³•
        assert hasattr(manager, 'run_spider')
        assert hasattr(manager, 'run_all_spiders')
        assert hasattr(manager, 'get_spider_list')
        assert hasattr(manager, 'get_spider_status')
        assert hasattr(manager, 'enable_spider')
        assert hasattr(manager, 'disable_spider')

        logger.info("âœ… çˆ¬è™«ç®¡ç†å™¨æ–¹æ³•éªŒè¯é€šè¿‡")

    def test_get_spider_list(self):
        """æµ‹è¯•11: è·å–çˆ¬è™«åˆ—è¡¨"""
        from backend.crawler.crawler_manager import get_crawler_manager

        manager = get_crawler_manager()
        spider_list = manager.get_spider_list()

        assert len(spider_list) >= 3  # è‡³å°‘3ä¸ªçˆ¬è™«

        # æ£€æŸ¥æ¯ä¸ªçˆ¬è™«çš„ä¿¡æ¯å®Œæ•´æ€§
        for spider_info in spider_list:
            assert 'name' in spider_info
            assert 'description' in spider_info
            assert 'type' in spider_info
            assert 'enabled' in spider_info
            assert 'priority' in spider_info

        logger.info(f"âœ… çˆ¬è™«åˆ—è¡¨éªŒè¯é€šè¿‡ï¼Œå…±{len(spider_list)}ä¸ªçˆ¬è™«")

    def test_crawler_integration(self):
        """æµ‹è¯•12: çˆ¬è™«æ¡†æ¶å®Œæ•´æ€§éªŒè¯"""
        # éªŒè¯æ‰€æœ‰æ ¸å¿ƒç»„ä»¶éƒ½å·²åˆ›å»º
        components = [
            project_root / "backend" / "crawler" / "spiders" / "gov_procurement_spider.py",
            project_root / "backend" / "crawler" / "spiders" / "dynamic_platform_spider.py",
            project_root / "backend" / "crawler" / "spiders" / "auth_platform_spider.py",
            project_root / "backend" / "crawler" / "incremental_manager.py",
            project_root / "backend" / "crawler" / "crawler_manager.py",
        ]

        for component in components:
            assert component.exists(), f"ç¼ºå°‘ç»„ä»¶: {component}"

        logger.info(f"âœ… çˆ¬è™«å¹³å°å¯¹æ¥å®Œæ•´æ€§éªŒè¯é€šè¿‡ - å…±{len(components)}ä¸ªç»„ä»¶")


if __name__ == "__main__":
    logger.info("ğŸ§ª å¼€å§‹è¿è¡Œçˆ¬è™«å¹³å°æµ‹è¯•...")
    pytest.main([__file__, "-v", "-s"])

