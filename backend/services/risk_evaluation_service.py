"""
é£é™©è¯†åˆ«å‡†ç¡®ç‡éªŒè¯æœåŠ¡
ç”¨äºè¯„ä¼°é£é™©è¯†åˆ«çš„å‡†ç¡®ç‡ã€å¬å›ç‡å’ŒF1åˆ†æ•°
"""

import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)


class RiskMatchType(str, Enum):
    """é£é™©åŒ¹é…ç±»å‹"""
    TRUE_POSITIVE = "true_positive"   # æ­£ç¡®è¯†åˆ«ï¼ˆé¢„æµ‹ä¸ºé£é™©ï¼Œå®é™…ä¸ºé£é™©ï¼‰
    FALSE_POSITIVE = "false_positive"  # è¯¯è¯†åˆ«ï¼ˆé¢„æµ‹ä¸ºé£é™©ï¼Œå®é™…ä¸æ˜¯ï¼‰
    TRUE_NEGATIVE = "true_negative"    # æ­£ç¡®æ’é™¤ï¼ˆé¢„æµ‹ä¸æ˜¯é£é™©ï¼Œå®é™…ä¸æ˜¯ï¼‰
    FALSE_NEGATIVE = "false_negative"  # æ¼è¯†åˆ«ï¼ˆé¢„æµ‹ä¸æ˜¯é£é™©ï¼Œå®é™…ä¸ºé£é™©ï¼‰


@dataclass
class GroundTruthRisk:
    """äººå·¥æ ‡æ³¨çš„çœŸå®é£é™©"""
    risk_id: str
    title: str
    risk_type: str
    risk_level: str
    page_number: int
    original_text: str
    section: Optional[str] = None


@dataclass
class PredictedRisk:
    """ç³»ç»Ÿé¢„æµ‹çš„é£é™©"""
    risk_id: str
    title: str
    risk_type: str
    risk_level: str
    page_number: int
    original_text: str
    confidence_score: int


@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    precision: float      # å‡†ç¡®ç‡ = TP / (TP + FP)
    recall: float         # å¬å›ç‡ = TP / (TP + FN)
    f1_score: float       # F1åˆ†æ•° = 2 * (precision * recall) / (precision + recall)
    accuracy: float       # æ•´ä½“å‡†ç¡®ç‡ = (TP + TN) / (TP + TN + FP + FN)

    true_positives: int   # æ­£ç¡®è¯†åˆ«æ•°é‡
    false_positives: int  # è¯¯è¯†åˆ«æ•°é‡
    false_negatives: int  # æ¼è¯†åˆ«æ•°é‡
    true_negatives: int   # æ­£ç¡®æ’é™¤æ•°é‡

    total_ground_truth: int   # çœŸå®é£é™©æ€»æ•°
    total_predicted: int      # é¢„æµ‹é£é™©æ€»æ•°


class RiskEvaluationService:
    """é£é™©è¯†åˆ«è¯„ä¼°æœåŠ¡"""

    def __init__(self, similarity_threshold: float = 0.7):
        """
        åˆå§‹åŒ–è¯„ä¼°æœåŠ¡

        Args:
            similarity_threshold: æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œç”¨äºåˆ¤æ–­ä¸¤ä¸ªé£é™©æ˜¯å¦åŒ¹é…
        """
        self.similarity_threshold = similarity_threshold

    def evaluate(
        self,
        ground_truth_risks: List[GroundTruthRisk],
        predicted_risks: List[PredictedRisk]
    ) -> EvaluationMetrics:
        """
        è¯„ä¼°é£é™©è¯†åˆ«ç»“æœ

        Args:
            ground_truth_risks: äººå·¥æ ‡æ³¨çš„çœŸå®é£é™©åˆ—è¡¨
            predicted_risks: ç³»ç»Ÿé¢„æµ‹çš„é£é™©åˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡
        """
        logger.info(f"ğŸ“Š å¼€å§‹è¯„ä¼°é£é™©è¯†åˆ«å‡†ç¡®ç‡...")
        logger.info(f"çœŸå®é£é™©æ•°é‡: {len(ground_truth_risks)}, é¢„æµ‹é£é™©æ•°é‡: {len(predicted_risks)}")

        # åŒ¹é…çœŸå®é£é™©å’Œé¢„æµ‹é£é™©
        matches = self._match_risks(ground_truth_risks, predicted_risks)

        # è®¡ç®—æŒ‡æ ‡
        tp = matches['true_positives']
        fp = matches['false_positives']
        fn = matches['false_negatives']
        tn = matches['true_negatives']

        # è®¡ç®—å‡†ç¡®ç‡ã€å¬å›ç‡ã€F1åˆ†æ•°
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0.0

        metrics = EvaluationMetrics(
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            accuracy=accuracy,
            true_positives=tp,
            false_positives=fp,
            false_negatives=fn,
            true_negatives=tn,
            total_ground_truth=len(ground_truth_risks),
            total_predicted=len(predicted_risks)
        )

        logger.info(f"âœ… è¯„ä¼°å®Œæˆ:")
        logger.info(f"  å‡†ç¡®ç‡ (Precision): {precision:.2%}")
        logger.info(f"  å¬å›ç‡ (Recall): {recall:.2%}")
        logger.info(f"  F1åˆ†æ•°: {f1_score:.2%}")
        logger.info(f"  æ•´ä½“å‡†ç¡®ç‡: {accuracy:.2%}")
        logger.info(f"  TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")

        return metrics

    def _match_risks(
        self,
        ground_truth_risks: List[GroundTruthRisk],
        predicted_risks: List[PredictedRisk]
    ) -> Dict[str, int]:
        """
        åŒ¹é…çœŸå®é£é™©å’Œé¢„æµ‹é£é™©

        Args:
            ground_truth_risks: çœŸå®é£é™©åˆ—è¡¨
            predicted_risks: é¢„æµ‹é£é™©åˆ—è¡¨

        Returns:
            åŒ¹é…ç»Ÿè®¡å­—å…¸
        """
        matched_ground_truth = set()
        matched_predicted = set()

        tp = 0  # True Positives
        fp = 0  # False Positives
        fn = 0  # False Negatives

        # ä¸ºæ¯ä¸ªé¢„æµ‹é£é™©å¯»æ‰¾æœ€ä½³åŒ¹é…çš„çœŸå®é£é™©
        for pred_risk in predicted_risks:
            best_match = None
            best_similarity = 0.0

            for gt_risk in ground_truth_risks:
                if gt_risk.risk_id in matched_ground_truth:
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼šåŸºäºé¡µç å’Œæ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰
                similarity = self._calculate_similarity(gt_risk, pred_risk)

                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_match = gt_risk

            if best_match:
                # æ‰¾åˆ°åŒ¹é…ï¼Œè®¡ä¸ºTrue Positive
                tp += 1
                matched_ground_truth.add(best_match.risk_id)
                matched_predicted.add(pred_risk.risk_id)
            else:
                # æœªæ‰¾åˆ°åŒ¹é…ï¼Œè®¡ä¸ºFalse Positiveï¼ˆè¯¯è¯†åˆ«ï¼‰
                fp += 1

        # æœªè¢«åŒ¹é…çš„çœŸå®é£é™©è®¡ä¸ºFalse Negativeï¼ˆæ¼è¯†åˆ«ï¼‰
        fn = len(ground_truth_risks) - len(matched_ground_truth)

        # True Negativeè¾ƒéš¾å®šä¹‰ï¼Œè¿™é‡Œå‡è®¾ä¸º0ï¼ˆå› ä¸ºæˆ‘ä»¬åªå…³æ³¨é£é™©æ¡æ¬¾ï¼‰
        tn = 0

        return {
            'true_positives': tp,
            'false_positives': fp,
            'false_negatives': fn,
            'true_negatives': tn
        }

    def _calculate_similarity(
        self,
        gt_risk: GroundTruthRisk,
        pred_risk: PredictedRisk
    ) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªé£é™©çš„ç›¸ä¼¼åº¦

        Args:
            gt_risk: çœŸå®é£é™©
            pred_risk: é¢„æµ‹é£é™©

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # 1. é¡µç åŒ¹é…ï¼ˆæƒé‡30%ï¼‰
        page_score = 1.0 if gt_risk.page_number == pred_risk.page_number else 0.0

        # 2. é£é™©ç±»å‹åŒ¹é…ï¼ˆæƒé‡30%ï¼‰
        type_score = 1.0 if gt_risk.risk_type == pred_risk.risk_type else 0.0

        # 3. æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆæƒé‡40%ï¼‰
        text_similarity = self._text_similarity(gt_risk.original_text, pred_risk.original_text)

        # åŠ æƒå¹³å‡
        similarity = 0.3 * page_score + 0.3 * type_score + 0.4 * text_similarity

        return similarity

    def _text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼šåŸºäºJaccardç›¸ä¼¼åº¦ï¼‰

        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not text1 or not text2:
            return 0.0

        # è½¬æ¢ä¸ºå­—ç¬¦é›†åˆ
        set1 = set(text1.replace(" ", "").replace("\n", ""))
        set2 = set(text2.replace(" ", "").replace("\n", ""))

        # è®¡ç®—Jaccardç›¸ä¼¼åº¦
        intersection = len(set1 & set2)
        union = len(set1 | set2)

        if union == 0:
            return 0.0

        return intersection / union

    def generate_report(
        self,
        metrics: EvaluationMetrics,
        misclassified_cases: Optional[List[Dict[str, Any]]] = None
    ) -> str:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š

        Args:
            metrics: è¯„ä¼°æŒ‡æ ‡
            misclassified_cases: è¯¯åˆ†ç±»æ¡ˆä¾‹åˆ—è¡¨

        Returns:
            è¯„ä¼°æŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        report = f"""# é£é™©è¯†åˆ«å‡†ç¡®ç‡è¯„ä¼°æŠ¥å‘Š

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

| æŒ‡æ ‡ | æ•°å€¼ | ç›®æ ‡ | è¾¾æ ‡ |
|------|------|------|------|
| **å‡†ç¡®ç‡ (Precision)** | {metrics.precision:.2%} | >85% | {'âœ…' if metrics.precision > 0.85 else 'âŒ'} |
| **å¬å›ç‡ (Recall)** | {metrics.recall:.2%} | >80% | {'âœ…' if metrics.recall > 0.80 else 'âŒ'} |
| **F1åˆ†æ•°** | {metrics.f1_score:.2%} | >82% | {'âœ…' if metrics.f1_score > 0.82 else 'âŒ'} |
| **æ•´ä½“å‡†ç¡®ç‡** | {metrics.accuracy:.2%} | - | - |

## ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡

- **çœŸå®é£é™©æ€»æ•°ï¼š** {metrics.total_ground_truth}
- **é¢„æµ‹é£é™©æ€»æ•°ï¼š** {metrics.total_predicted}
- **æ­£ç¡®è¯†åˆ« (TP)ï¼š** {metrics.true_positives}
- **è¯¯è¯†åˆ« (FP)ï¼š** {metrics.false_positives}
- **æ¼è¯†åˆ« (FN)ï¼š** {metrics.false_negatives}

## ğŸ¯ è¯¯è¯†åˆ«ç‡

- **è¯¯è¯†åˆ«ç‡ï¼š** {(metrics.false_positives / metrics.total_predicted * 100) if metrics.total_predicted > 0 else 0:.2f}%
- **æ¼è¯†åˆ«ç‡ï¼š** {(metrics.false_negatives / metrics.total_ground_truth * 100) if metrics.total_ground_truth > 0 else 0:.2f}%

## ğŸ“ ç»“è®º

{'âœ… **éªŒæ”¶é€šè¿‡** - æ‰€æœ‰æŒ‡æ ‡å‡è¾¾åˆ°é¢„æœŸç›®æ ‡' if metrics.precision > 0.85 and metrics.recall > 0.80 else 'âš ï¸ **éœ€è¦ä¼˜åŒ–** - éƒ¨åˆ†æŒ‡æ ‡æœªè¾¾æ ‡ï¼Œå»ºè®®è¿›è¡Œæ¨¡å‹è°ƒä¼˜'}
"""

        if misclassified_cases:
            report += f"\n\n## âš ï¸ è¯¯åˆ†ç±»æ¡ˆä¾‹\n\nå…± {len(misclassified_cases)} ä¸ªæ¡ˆä¾‹éœ€è¦åˆ†æ\n"

        return report


# å•ä¾‹å®ä¾‹
_risk_evaluation_service = None


def get_risk_evaluation_service() -> RiskEvaluationService:
    """è·å–é£é™©è¯„ä¼°æœåŠ¡å•ä¾‹"""
    global _risk_evaluation_service
    if _risk_evaluation_service is None:
        _risk_evaluation_service = RiskEvaluationService()
    return _risk_evaluation_service

