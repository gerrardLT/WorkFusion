# Agentic RAG ç³»ç»Ÿæ•…éšœæ’æŸ¥æ‰‹å†Œ

## ğŸ“‹ ç›®å½•

- [1. ç³»ç»Ÿå¯åŠ¨é—®é¢˜](#1-ç³»ç»Ÿå¯åŠ¨é—®é¢˜)
- [2. æ–‡æ¡£å¤„ç†é—®é¢˜](#2-æ–‡æ¡£å¤„ç†é—®é¢˜)
- [3. æ£€ç´¢é—®é¢˜](#3-æ£€ç´¢é—®é¢˜)
- [4. APIè°ƒç”¨é—®é¢˜](#4-apiè°ƒç”¨é—®é¢˜)
- [5. ç¼“å­˜é—®é¢˜](#5-ç¼“å­˜é—®é¢˜)
- [6. æ€§èƒ½é—®é¢˜](#6-æ€§èƒ½é—®é¢˜)
- [7. æ•°æ®åº“é—®é¢˜](#7-æ•°æ®åº“é—®é¢˜)
- [8. å‰ç«¯é—®é¢˜](#8-å‰ç«¯é—®é¢˜)
- [9. å¸¸è§é”™è¯¯ç ](#9-å¸¸è§é”™è¯¯ç )
- [10. è°ƒè¯•æŠ€å·§](#10-è°ƒè¯•æŠ€å·§)

---

## 1. ç³»ç»Ÿå¯åŠ¨é—®é¢˜

### 1.1 åç«¯å¯åŠ¨å¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: ModuleNotFoundError: No module named 'xxx'
```

**åŸå› **: ä¾èµ–åŒ…æœªå®‰è£…

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
.\venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python -c "import faiss; print('FAISS OK')"
python -c "import dashscope; print('DashScope OK')"
```

---

### 1.2 APIå¯†é’¥é”™è¯¯

**ç—‡çŠ¶**:
```
ERROR: Invalid API key
```

**åŸå› **: DashScope APIå¯†é’¥æœªé…ç½®æˆ–æ— æ•ˆ

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥ .env æ–‡ä»¶
cat .env | grep DASHSCOPE_API_KEY

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export DASHSCOPE_API_KEY=your_api_key_here  # Linux/Mac
$env:DASHSCOPE_API_KEY="your_api_key_here"  # Windows PowerShell

# 3. éªŒè¯APIå¯†é’¥
python -c "from config import get_settings; print(get_settings().dashscope_api_key)"
```

---

### 1.3 æ•°æ®åº“è¿æ¥å¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: Unable to open database file
```

**åŸå› **: æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨æˆ–æƒé™ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
ls -la data/stock_data/databases/stock_rag.db

# 2. åˆ›å»ºæ•°æ®åº“ç›®å½•
mkdir -p data/stock_data/databases

# 3. åˆå§‹åŒ–æ•°æ®åº“
python -c "from backend.database import init_db; init_db()"

# 4. æ£€æŸ¥æƒé™
chmod 644 data/stock_data/databases/stock_rag.db  # Linux/Mac
```

---

### 1.4 ç«¯å£è¢«å ç”¨

**ç—‡çŠ¶**:
```
ERROR: [Errno 48] Address already in use
```

**åŸå› **: ç«¯å£8000æˆ–3005å·²è¢«å ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
# Windows
netstat -ano | findstr :8000
taskkill /PID <PID> /F

# Linux/Mac
lsof -i :8000
kill -9 <PID>

# æˆ–è€…ä½¿ç”¨å…¶ä»–ç«¯å£
uvicorn backend.main_multi_scenario:app --port 8001
```

---

## 2. æ–‡æ¡£å¤„ç†é—®é¢˜

### 2.1 PDFè§£æå¤±è´¥

**ç—‡çŠ¶**:
```
WARNING: MinerU (magic-pdf) not available, using fallback PDF parsing
```

**åŸå› **: MinerUæœªæ­£ç¡®å®‰è£…

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥MinerUå‘½ä»¤
which magic-pdf  # Linux/Mac
where magic-pdf  # Windows

# 2. å®‰è£…MinerU
pip install mineru[core]

# 3. éªŒè¯å®‰è£…
magic-pdf --version

# 4. å¦‚æœä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥å¤‡ç”¨è§£æå™¨
python -c "import fitz; print('PyMuPDF OK')"
python -c "import pdfplumber; print('pdfplumber OK')"
```

---

### 2.2 FAISSå‘é‡åŒ–å¤±è´¥ï¼ˆä¸­æ–‡è·¯å¾„ï¼‰

**ç—‡çŠ¶**:
```
ERROR: Error in faiss::FileIOWriter: Illegal byte sequence
```

**åŸå› **: FAISSæ— æ³•å¤„ç†åŒ…å«ä¸­æ–‡çš„æ–‡ä»¶åæˆ–è·¯å¾„

**è§£å†³æ–¹æ¡ˆ**:
```python
# å·²åœ¨ src/ingestion.py ä¸­å®ç°
# ä½¿ç”¨MD5 hashç”Ÿæˆå®‰å…¨çš„ä¸´æ—¶æ–‡ä»¶å

import hashlib
import tempfile
from pathlib import Path

# ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
safe_filename = hashlib.md5(file_id.encode()).hexdigest()
temp_dir = Path(tempfile.mkdtemp(prefix="faiss_temp_"))
temp_faiss_file = temp_dir / f"{safe_filename}_vector.faiss"

# å†™å…¥ä¸´æ—¶æ–‡ä»¶
faiss.write_index(index, str(temp_faiss_file))

# å¤åˆ¶åˆ°æœ€ç»ˆä½ç½®
shutil.copy2(temp_faiss_file, final_path)
```

**éªŒè¯**:
```bash
# æ£€æŸ¥å‘é‡æ–‡ä»¶æ˜¯å¦ç”Ÿæˆ
ls -la data/stock_data/databases/vector_dbs/
```

---

### 2.3 æ–‡æ¡£åˆ†å—å¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: 'pages' key not found in parsed report
```

**åŸå› **: PDFè§£æç»“æœæ ¼å¼ä¸æ­£ç¡®

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥è§£æç»“æœæ ¼å¼
import json
with open("data/debug_data/parsed_reports/xxx_parsed.json") as f:
    data = json.load(f)
    print(data.keys())  # åº”è¯¥åŒ…å« 'pages'

# æ­£ç¡®çš„æ ¼å¼
{
    "file_id": "...",
    "pages": [
        {
            "page_number": 1,
            "text": "é¡µé¢å†…å®¹...",
            "metadata": {...}
        }
    ]
}
```

---

### 2.4 BM25ç´¢å¼•æ„å»ºå¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: BM25 index creation failed
```

**åŸå› **: æ–‡æœ¬å—ä¸ºç©ºæˆ–åˆ†è¯å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥æ–‡æœ¬å—
from src.ingestion import IngestionPipeline

pipeline = IngestionPipeline()
chunks = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2"]

# 2. æµ‹è¯•åˆ†è¯
bm25_index = pipeline.create_bm25_index(chunks)
print(f"ç´¢å¼•åˆ›å»ºæˆåŠŸ: {bm25_index is not None}")

# 3. æ£€æŸ¥ç´¢å¼•æ–‡ä»¶
import os
bm25_files = os.listdir("data/stock_data/databases/bm25/")
print(f"BM25ç´¢å¼•æ–‡ä»¶: {bm25_files}")
```

---

## 3. æ£€ç´¢é—®é¢˜

### 3.1 æ£€ç´¢æ— ç»“æœ

**ç—‡çŠ¶**:
```
WARNING: No documents found for query
```

**åŸå› **: ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨æˆ–ä¸ºç©º

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥ç´¢å¼•æ–‡ä»¶
import os
from pathlib import Path

bm25_dir = Path("data/stock_data/databases/bm25/")
vector_dir = Path("data/stock_data/databases/vector_dbs/")

print(f"BM25ç´¢å¼•: {list(bm25_dir.glob('*.pkl'))}")
print(f"FAISSç´¢å¼•: {list(vector_dir.glob('*.faiss'))}")

# 2. é‡å»ºç´¢å¼•
from src.pipeline import Pipeline, RunConfig

pipeline = Pipeline(
    root_path=Path("data/stock_data"),
    scenario_id="tender",
    run_config=RunConfig()
)

result = pipeline.prepare_documents(force_rebuild=True)
print(f"æ–‡æ¡£å‡†å¤‡ç»“æœ: {result}")
```

---

### 3.2 æ£€ç´¢é€Ÿåº¦æ…¢

**ç—‡çŠ¶**: æ£€ç´¢è€—æ—¶ > 5ç§’

**åŸå› **: ç´¢å¼•æœªé¢„åŠ è½½æˆ–æ–‡æ¡£æ•°é‡è¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¯ç”¨ç¼“å­˜
from src.cache.smart_cache import SmartCache

cache = SmartCache(max_size=1000)
cached_result = cache.get("æŸ¥è¯¢é—®é¢˜")

# 2. å‡å°‘æ£€ç´¢æ•°é‡
from src.retrieval.hybrid_retriever import HybridRetriever

retriever = HybridRetriever(scenario_id="tender")
results = retriever.retrieve(
    question="...",
    top_k=5  # å‡å°‘åˆ°5ä¸ª
)

# 3. ä½¿ç”¨æŸ¥è¯¢å‘é‡ç¼“å­˜
query_vector_cache = {}

def get_cached_embedding(query: str):
    if query in query_vector_cache:
        return query_vector_cache[query]

    embedding = api_processor.get_embeddings([query])[0]
    query_vector_cache[query] = embedding
    return embedding
```

---

### 3.3 æ£€ç´¢ç»“æœä¸ç›¸å…³

**ç—‡çŠ¶**: è¿”å›çš„æ–‡æ¡£ä¸é—®é¢˜æ— å…³

**åŸå› **: æƒé‡é…ç½®ä¸åˆç†æˆ–é˜ˆå€¼è¿‡ä½

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è°ƒæ•´æƒé‡
retriever.retrieve(
    question="...",
    bm25_weight=0.6,  # æé«˜å…³é”®è¯æƒé‡
    vector_weight=0.4
)

# 2. æé«˜ç›¸ä¼¼åº¦é˜ˆå€¼
vector_retriever.search(
    query="...",
    min_similarity=0.6  # æé«˜åˆ°0.6
)

# 3. ä½¿ç”¨æ™ºèƒ½è·¯ç”±
from src.agents.routing_agent import RoutingAgent

agent = RoutingAgent(scenario_id="tender")
result = agent.route_documents(
    chunks=candidates,
    question="...",
    top_k=5
)
```

---

## 4. APIè°ƒç”¨é—®é¢˜

### 4.1 APIè°ƒç”¨è¶…æ—¶

**ç—‡çŠ¶**:
```
ERROR: Request timeout after 30 seconds
```

**åŸå› **: ç½‘ç»œé—®é¢˜æˆ–APIæœåŠ¡ç¹å¿™

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
from api_requests import APIProcessor

api_processor = APIProcessor(
    provider="dashscope",
    timeout=60  # å¢åŠ åˆ°60ç§’
)

# 2. å¯ç”¨é‡è¯•æœºåˆ¶
import time

def api_call_with_retry(func, max_retries=3):
    for i in range(max_retries):
        try:
            return func()
        except Exception as e:
            if i == max_retries - 1:
                raise
            print(f"é‡è¯• {i+1}/{max_retries}...")
            time.sleep(2 ** i)  # æŒ‡æ•°é€€é¿

# 3. æ£€æŸ¥ç½‘ç»œè¿æ¥
import requests
response = requests.get("https://dashscope.aliyuncs.com")
print(f"APIæœåŠ¡çŠ¶æ€: {response.status_code}")
```

---

### 4.2 APIé…é¢è¶…é™

**ç—‡çŠ¶**:
```
ERROR: Rate limit exceeded
```

**åŸå› **: APIè°ƒç”¨é¢‘ç‡è¿‡é«˜

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¯ç”¨æ™ºèƒ½å»¶è¿Ÿ
import time

def rate_limited_call(func, delay=0.5):
    result = func()
    time.sleep(delay)
    return result

# 2. ä½¿ç”¨æ‰¹å¤„ç†
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3"]
embeddings = api_processor.get_embeddings(texts)  # æ‰¹é‡è°ƒç”¨

# 3. å¯ç”¨ç¼“å­˜å‡å°‘è°ƒç”¨
from src.cache.smart_cache import SmartCache
cache = SmartCache()

# å…ˆæ£€æŸ¥ç¼“å­˜
result = cache.get(question)
if not result:
    # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨API
    result = process_question(question)
    cache.set(question, result)
```

---

### 4.3 åµŒå…¥å‘é‡ç”Ÿæˆå¤±è´¥

**ç—‡çŠ¶**:
```
ERROR: Failed to generate embeddings
```

**åŸå› **: æ–‡æœ¬è¿‡é•¿æˆ–æ ¼å¼é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥æ–‡æœ¬é•¿åº¦
def check_text_length(text: str, max_length: int = 8000):
    if len(text) > max_length:
        print(f"è­¦å‘Š: æ–‡æœ¬è¿‡é•¿ ({len(text)} å­—ç¬¦)")
        return text[:max_length]
    return text

# 2. æ¸…ç†æ–‡æœ¬
def clean_text(text: str) -> str:
    # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = text.replace('\x00', '')
    # ç§»é™¤è¿‡å¤šç©ºç™½
    text = ' '.join(text.split())
    return text

# 3. åˆ†æ‰¹å¤„ç†
def batch_embed(texts: List[str], batch_size: int = 10):
    results = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        embeddings = api_processor.get_embeddings(batch)
        results.extend(embeddings)
    return results
```

---

## 5. ç¼“å­˜é—®é¢˜

### 5.1 ç¼“å­˜æœªå‘½ä¸­

**ç—‡çŠ¶**: ç¼“å­˜å‘½ä¸­ç‡ < 10%

**åŸå› **: ç¼“å­˜é…ç½®ä¸åˆç†æˆ–é—®é¢˜å˜åŒ–å¤§

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. é™ä½è¯­ä¹‰é˜ˆå€¼
cache = SmartCache(
    semantic_threshold=0.90  # ä»0.95é™ä½åˆ°0.90
)

# 2. å¢åŠ ç¼“å­˜å¤§å°
cache = SmartCache(
    max_size=5000  # å¢åŠ åˆ°5000
)

# 3. å»¶é•¿TTL
cache = SmartCache(
    exact_ttl_days=14,    # å¢åŠ åˆ°14å¤©
    semantic_ttl_days=7   # å¢åŠ åˆ°7å¤©
)

# 4. ç¼“å­˜é¢„çƒ­
common_questions = [
    "é¡¹ç›®é¢„ç®—æ˜¯å¤šå°‘ï¼Ÿ",
    "æˆªæ­¢æ—¥æœŸæ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿ"
]

for q in common_questions:
    result = process_question(q)
    cache.set(q, result, use_semantic=True)
```

---

### 5.2 ç¼“å­˜å ç”¨å†…å­˜è¿‡å¤§

**ç—‡çŠ¶**: ç³»ç»Ÿå†…å­˜å ç”¨ > 2GB

**åŸå› **: ç¼“å­˜æ¡ç›®è¿‡å¤šæˆ–å•æ¡ç›®è¿‡å¤§

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å‡å°ç¼“å­˜å¤§å°
cache = SmartCache(max_size=500)

# 2. åªç¼“å­˜å¿…è¦å­—æ®µ
def cache_answer(question: str, result: Dict):
    # åªç¼“å­˜æ ¸å¿ƒå­—æ®µ
    cached_data = {
        "answer": result["answer"],
        "confidence": result["confidence"],
        "sources": result["sources"][:3]  # åªç¼“å­˜å‰3ä¸ªæ¥æº
    }
    cache.set(question, cached_data)

# 3. å®šæœŸæ¸…ç†
def cleanup_cache():
    stats = cache.get_stats()
    if stats["total_cache_size"] > 1000:
        # æ¸…ç©ºç¼“å­˜
        cache = SmartCache()
```

---

### 5.3 è¯­ä¹‰ç¼“å­˜é€Ÿåº¦æ…¢

**ç—‡çŠ¶**: è¯­ä¹‰ç¼“å­˜æŸ¥è¯¢ > 1ç§’

**åŸå› **: éå†æ‰€æœ‰ç¼“å­˜é¡¹è®¡ç®—ç›¸ä¼¼åº¦

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨FAISSåŠ é€Ÿè¯­ä¹‰ç¼“å­˜
import faiss
import numpy as np

class FastSemanticCache:
    def __init__(self):
        self.dimension = 1536
        self.index = faiss.IndexFlatIP(self.dimension)
        self.cache_data = []

    def add(self, question: str, answer: Dict):
        # è·å–é—®é¢˜å‘é‡
        embedding = get_embedding(question)

        # æ·»åŠ åˆ°FAISSç´¢å¼•
        self.index.add(np.array([embedding]))
        self.cache_data.append({
            "question": question,
            "answer": answer
        })

    def search(self, question: str, threshold: float = 0.95):
        embedding = get_embedding(question)

        # FAISSå¿«é€Ÿæœç´¢
        D, I = self.index.search(np.array([embedding]), k=1)

        if D[0][0] > threshold:
            return self.cache_data[I[0][0]]["answer"]
        return None
```

---

## 6. æ€§èƒ½é—®é¢˜

### 6.1 å“åº”æ—¶é—´è¿‡é•¿

**ç—‡çŠ¶**: å¹³å‡å“åº”æ—¶é—´ > 10ç§’

**åŸå› **: æ£€ç´¢ã€LLMè°ƒç”¨æˆ–éªŒè¯ç¯èŠ‚è€—æ—¶è¿‡é•¿

**è¯Šæ–­**:
```python
import time

def profile_pipeline(question: str):
    start = time.time()

    # 1. æ£€ç´¢
    t1 = time.time()
    chunks = retriever.retrieve(question)
    retrieval_time = time.time() - t1

    # 2. è·¯ç”±
    t2 = time.time()
    routed = agent.route_documents(chunks, question)
    routing_time = time.time() - t2

    # 3. ç”Ÿæˆç­”æ¡ˆ
    t3 = time.time()
    answer = generate_answer(question, routed)
    generation_time = time.time() - t3

    # 4. éªŒè¯
    t4 = time.time()
    verified = verifier.verify_answer(answer, routed, question)
    verification_time = time.time() - t4

    total_time = time.time() - start

    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"  æ£€ç´¢: {retrieval_time:.2f}ç§’ ({retrieval_time/total_time*100:.1f}%)")
    print(f"  è·¯ç”±: {routing_time:.2f}ç§’ ({routing_time/total_time*100:.1f}%)")
    print(f"  ç”Ÿæˆ: {generation_time:.2f}ç§’ ({generation_time/total_time*100:.1f}%)")
    print(f"  éªŒè¯: {verification_time:.2f}ç§’ ({verification_time/total_time*100:.1f}%)")
```

**ä¼˜åŒ–æ–¹æ¡ˆ**:
```python
# 1. å¯ç”¨ç¼“å­˜
cache = SmartCache(max_size=1000)

# 2. å‡å°‘æ£€ç´¢æ•°é‡
retriever.retrieve(question, top_k=5)

# 3. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹
api_processor.llm_model = "qwen-turbo-latest"

# 4. è·³è¿‡éªŒè¯ï¼ˆå¦‚æœå¯æ¥å—ï¼‰
skip_verification = True
```

---

### 6.2 å¹¶å‘æ€§èƒ½å·®

**ç—‡çŠ¶**: å¹¶å‘è¯·æ±‚ > 10 æ—¶å“åº”å˜æ…¢

**åŸå› **: å•çº¿ç¨‹å¤„ç†ï¼Œæ— å¹¶å‘ä¼˜åŒ–

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨å¼‚æ­¥å¤„ç†
import asyncio

async def process_question_async(question: str):
    # å¼‚æ­¥æ£€ç´¢
    chunks = await retriever.retrieve_async(question)

    # å¼‚æ­¥ç”Ÿæˆç­”æ¡ˆ
    answer = await generate_answer_async(question, chunks)

    return answer

# 2. ä½¿ç”¨çº¿ç¨‹æ± 
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=10)

def process_batch(questions: List[str]):
    futures = [
        executor.submit(process_question, q)
        for q in questions
    ]

    results = [f.result() for f in futures]
    return results

# 3. ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆCPUå¯†é›†å‹ï¼‰
from multiprocessing import Pool

with Pool(processes=4) as pool:
    results = pool.map(process_question, questions)
```

---

### 6.3 å†…å­˜å ç”¨è¿‡é«˜

**ç—‡çŠ¶**: ç³»ç»Ÿå†…å­˜ > 4GB

**åŸå› **: ç´¢å¼•æœªé‡Šæ”¾æˆ–ç¼“å­˜è¿‡å¤§

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å®šæœŸé‡Šæ”¾å†…å­˜
import gc

def cleanup_memory():
    gc.collect()

# 2. ä½¿ç”¨å†…å­˜æ˜ å°„ï¼ˆFAISSï¼‰
index = faiss.read_index("index.faiss", faiss.IO_FLAG_MMAP)

# 3. é™åˆ¶ç¼“å­˜å¤§å°
cache = SmartCache(max_size=500)

# 4. ç›‘æ§å†…å­˜ä½¿ç”¨
import psutil

def check_memory():
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f"å†…å­˜ä½¿ç”¨: {mem_info.rss / 1024 / 1024:.2f} MB")
```

---

## 7. æ•°æ®åº“é—®é¢˜

### 7.1 æ•°æ®åº“é”å®š

**ç—‡çŠ¶**:
```
ERROR: database is locked
```

**åŸå› **: å¤šä¸ªè¿›ç¨‹åŒæ—¶å†™å…¥SQLite

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
from sqlalchemy import create_engine

engine = create_engine(
    "sqlite:///data/stock_rag.db",
    connect_args={"timeout": 30}
)

# 2. ä½¿ç”¨WALæ¨¡å¼
import sqlite3
conn = sqlite3.connect("data/stock_rag.db")
conn.execute("PRAGMA journal_mode=WAL")

# 3. ä½¿ç”¨è¿æ¥æ± 
from sqlalchemy.pool import QueuePool

engine = create_engine(
    "sqlite:///data/stock_rag.db",
    poolclass=QueuePool,
    pool_size=5,
    max_overflow=10
)
```

---

### 7.2 æ•°æ®åº“æŸå

**ç—‡çŠ¶**:
```
ERROR: database disk image is malformed
```

**åŸå› **: æ•°æ®åº“æ–‡ä»¶æŸå

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. å¤‡ä»½æ•°æ®åº“
cp data/stock_rag.db data/stock_rag.db.backup

# 2. å°è¯•ä¿®å¤
sqlite3 data/stock_rag.db "PRAGMA integrity_check;"

# 3. å¯¼å‡ºæ•°æ®
sqlite3 data/stock_rag.db ".dump" > backup.sql

# 4. é‡å»ºæ•°æ®åº“
rm data/stock_rag.db
sqlite3 data/stock_rag.db < backup.sql

# 5. å¦‚æœæ— æ³•ä¿®å¤ï¼Œä»å¤‡ä»½æ¢å¤
cp data/stock_rag.db.backup data/stock_rag.db
```

---

## 8. å‰ç«¯é—®é¢˜

### 8.1 åœºæ™¯åˆ‡æ¢ä¸ç”Ÿæ•ˆ

**ç—‡çŠ¶**: åˆ‡æ¢åœºæ™¯åUIæœªæ›´æ–°

**åŸå› **: å‰ç«¯çŠ¶æ€æœªåŒæ­¥

**è§£å†³æ–¹æ¡ˆ**:
```typescript
// 1. æ¸…é™¤æµè§ˆå™¨ç¼“å­˜
// Chrome: Ctrl+Shift+Delete

// 2. é‡å¯å‰ç«¯æœåŠ¡
npm run dev

// 3. æ£€æŸ¥åœºæ™¯é…ç½®
const { currentScenario } = useScenario();
console.log('å½“å‰åœºæ™¯:', currentScenario);

// 4. å¼ºåˆ¶åˆ·æ–°
window.location.reload();
```

---

### 8.2 æ–‡ä»¶ä¸Šä¼ å¤±è´¥

**ç—‡çŠ¶**: ä¸Šä¼ æŒ‰é’®æ— å“åº”æˆ–æŠ¥é”™

**åŸå› **: æ–‡ä»¶å¤§å°è¶…é™æˆ–æ ¼å¼ä¸æ”¯æŒ

**è§£å†³æ–¹æ¡ˆ**:
```typescript
// 1. æ£€æŸ¥æ–‡ä»¶å¤§å°
const MAX_FILE_SIZE = 10 * 1024 * 1024; // 10MB

if (file.size > MAX_FILE_SIZE) {
  alert('æ–‡ä»¶è¿‡å¤§ï¼Œè¯·é€‰æ‹©å°äº10MBçš„æ–‡ä»¶');
  return;
}

// 2. æ£€æŸ¥æ–‡ä»¶ç±»å‹
const ALLOWED_TYPES = ['.pdf', '.txt', '.md'];
const fileExt = file.name.substring(file.name.lastIndexOf('.'));

if (!ALLOWED_TYPES.includes(fileExt)) {
  alert('ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹');
  return;
}

// 3. æ£€æŸ¥åç«¯æ¥å£
fetch('http://localhost:8000/api/v2/upload', {
  method: 'POST',
  body: formData
})
.then(res => res.json())
.then(data => console.log(data))
.catch(err => console.error('ä¸Šä¼ å¤±è´¥:', err));
```

---

## 9. å¸¸è§é”™è¯¯ç 

| é”™è¯¯ç  | è¯´æ˜ | è§£å†³æ–¹æ¡ˆ |
|--------|------|----------|
| `PIPELINE_NOT_READY` | Pipelineæœªå°±ç»ª | è°ƒç”¨ `prepare_documents()` |
| `NO_DOCUMENTS_FOUND` | æœªæ‰¾åˆ°æ–‡æ¡£ | æ£€æŸ¥ç´¢å¼•æ–‡ä»¶ |
| `API_CALL_FAILED` | APIè°ƒç”¨å¤±è´¥ | æ£€æŸ¥ç½‘ç»œå’Œå¯†é’¥ |
| `CACHE_ERROR` | ç¼“å­˜é”™è¯¯ | æ¸…ç©ºç¼“å­˜ |
| `VERIFICATION_FAILED` | éªŒè¯å¤±è´¥ | æ£€æŸ¥æºæ–‡æ¡£ |
| `DATABASE_LOCKED` | æ•°æ®åº“é”å®š | å¢åŠ è¶…æ—¶æ—¶é—´ |
| `INVALID_SCENARIO` | æ— æ•ˆåœºæ™¯ID | æ£€æŸ¥åœºæ™¯é…ç½® |
| `EMBEDDING_FAILED` | å‘é‡ç”Ÿæˆå¤±è´¥ | æ£€æŸ¥æ–‡æœ¬æ ¼å¼ |

---

## 10. è°ƒè¯•æŠ€å·§

### 10.1 å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# é’ˆå¯¹ç‰¹å®šæ¨¡å—
logging.getLogger('src.retrieval').setLevel(logging.DEBUG)
logging.getLogger('src.agents').setLevel(logging.DEBUG)
```

---

### 10.2 ä½¿ç”¨è°ƒè¯•æ–­ç‚¹

```python
# åœ¨å…³é”®ä½ç½®æ·»åŠ æ–­ç‚¹
import pdb

def process_question(question: str):
    # æ£€ç´¢
    chunks = retriever.retrieve(question)

    # è°ƒè¯•ç‚¹
    pdb.set_trace()  # ç¨‹åºä¼šåœ¨è¿™é‡Œæš‚åœ

    # ç»§ç»­æ‰§è¡Œ
    answer = generate_answer(question, chunks)
    return answer
```

---

### 10.3 æ€§èƒ½åˆ†æ

```python
import cProfile
import pstats

# æ€§èƒ½åˆ†æ
profiler = cProfile.Profile()
profiler.enable()

# æ‰§è¡Œä»£ç 
result = process_question("æµ‹è¯•é—®é¢˜")

profiler.disable()

# æŸ¥çœ‹ç»“æœ
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(20)  # æ‰“å°å‰20ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
```

---

### 10.4 å†…å­˜åˆ†æ

```python
from memory_profiler import profile

@profile
def process_question(question: str):
    # å‡½æ•°ä»£ç 
    pass

# è¿è¡Œæ—¶ä¼šæ˜¾ç¤ºæ¯è¡Œä»£ç çš„å†…å­˜ä½¿ç”¨
```

---

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœä»¥ä¸Šæ–¹æ³•æ— æ³•è§£å†³é—®é¢˜ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**: `logs/system.log`
2. **æ£€æŸ¥æ–‡æ¡£**: `docs/` ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£
3. **è¿è¡Œæµ‹è¯•**: `python tests/test_agentic_rag_integration.py`
4. **è”ç³»å›¢é˜Ÿ**: æä¾›è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œæ—¥å¿—

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-09-30
**ç»´æŠ¤å›¢é˜Ÿ**: Agentic RAG å¼€å‘å›¢é˜Ÿ

